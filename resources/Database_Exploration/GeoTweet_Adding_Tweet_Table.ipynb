{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project 3 - GeoTweet+\n",
    "# \n",
    "# @Author Jeffery Brown (daddyjab)\n",
    "# @Date 5/1/19\n",
    "# @File GeoTweet_Adding_Tweet_Table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code: app.py - Initial Flask App Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: PostgreSQL database login/password is *not* populated\n"
     ]
    }
   ],
   "source": [
    "# Project 3 - GeoTweet+\n",
    "# \n",
    "# @Author Jeffery Brown (daddyjab)\n",
    "# @Date 5/1/19\n",
    "# @File app.py\n",
    "\n",
    "\n",
    "# import necessary libraries\n",
    "import os\n",
    "from flask import Flask, render_template, jsonify, request, redirect\n",
    "\n",
    "# Import Flask_CORS extension to enable Cross Origin Resource Sharing (CORS)\n",
    "# when deployed on Heroku\n",
    "from flask_cors import CORS\n",
    "\n",
    "#################################################\n",
    "# Flask Setup\n",
    "#################################################\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Enable Tracking of Flask-SQLAlchemy events for now (probably not needed)\n",
    "app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = True\n",
    "\n",
    "# Provide cross origin resource sharing\n",
    "CORS(app)\n",
    "\n",
    "#################################################\n",
    "# Database Setup\n",
    "#################################################\n",
    "\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "from sqlalchemy.sql.expression import func, and_, or_\n",
    "from sqlalchemy.sql.functions import coalesce\n",
    "\n",
    "#Probably don't need these from SQLAlchemy: asc, desc, between, distinct, func, null, nullsfirst, nullslast, or_, and_, not_\n",
    "\n",
    "# Local DB path for SQLite - default\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:    db_path_flask_app = \"sqlite:///data/twitter_trends.db\"\n",
    "# CODE TO RUN IN JUPYTER NOTEBOOK\n",
    "db_path_flask_app = \"sqlite:///../../python/data/twitter_trends.db\"\n",
    "\n",
    "# Local DB path for PostgreSQL - use only if login/password populated\n",
    "try:\n",
    "    # PostgreSQL Database Login/Password  \n",
    "    # -- only needed if using a local PostgresSQL instance (vs. SQLite)\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:    from .api_config import (postgres_geotweetapp_login, postgres_geotweetapp_password)\n",
    "# CODE TO RUN IN JUPYTER NOTEBOOK\n",
    "    from api_config import (postgres_geotweetapp_login, postgres_geotweetapp_password)\n",
    "\n",
    "    # If the login and password is populated\n",
    "    if (postgres_geotweetapp_login is not None) and (postgres_geotweetapp_password is not None):\n",
    "        db_path_flask_app = f\"postgresql://{postgres_geotweetapp_login}:{postgres_geotweetapp_password}@localhost/twitter_trends\"\n",
    "        print(\"Note: PostgreSQL database login/password is populated\")\n",
    "\n",
    "# If the api_config file is not available, then all we can do is flag an error\n",
    "except ImportError:\n",
    "    print(\"Note: PostgreSQL database login/password is *not* populated\")\n",
    "\n",
    "app.config['SQLALCHEMY_DATABASE_URI'] = os.environ.get('DATABASE_URL', '') or db_path_flask_app\n",
    "\n",
    "# Flask-SQLAlchemy database\n",
    "db = SQLAlchemy(app)\n",
    "\n",
    "# Import the schema for the Location and Trend tables needed for\n",
    "# 'twitter_trends.sqlite' database tables 'locations' and 'trends'\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK: from .models import (Location, Trend)\n",
    "\n",
    "# Import database management functions needed# to update the\n",
    "# 'twitter_trends.sqlite' database tables 'locations' and 'trends'\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK: from .db_management import (\n",
    "#     api_rate_limits, api_calls_remaining, api_time_before_reset,\n",
    "#     update_db_locations_table, update_db_trends_table,\n",
    "#     parse_date_range\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code: models.py - SQLAlchemy Models for Tables\n",
    "# In app.py: `from .models import (Location, Trend)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project 3 - GeoTweet+\n",
    "# \n",
    "# @Author Jeffery Brown (daddyjab)\n",
    "# @Date 5/1/19\n",
    "# @File models.py\n",
    "\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK: from .app import db\n",
    "\n",
    "# Database schema for Twitter 'locations' table\n",
    "class Location(db.Model):\n",
    "    __tablename__ = 'locations'\n",
    "    \n",
    "    # Defining the columns for the table 'locations',\n",
    "    # which will hold all of the locations in the U.S. for which\n",
    "    # top trends data is available, as well as location specific\n",
    "    # info like latitude/longitude\n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    updated_at = db.Column( db.DateTime )\n",
    "    woeid = db.Column(db.Integer, unique=True, nullable=False)\n",
    "    twitter_country = db.Column(db.String(100))\n",
    "    tritter_country_code = db.Column(db.String(10))\n",
    "    twitter_name = db.Column(db.String(250))\n",
    "    twitter_parentid = db.Column(db.Integer)\n",
    "    twitter_type = db.Column(db.String(50))\n",
    "    country_name = db.Column(db.String(250))\n",
    "    country_name_only = db.Column(db.String(250))\n",
    "    country_woeid = db.Column(db.Integer)\n",
    "    county_name = db.Column(db.String(250))\n",
    "    county_name_only = db.Column(db.String(250))\n",
    "    county_woeid = db.Column(db.Integer)\n",
    "    latitude = db.Column(db.Float)\n",
    "    longitude = db.Column(db.Float)\n",
    "    name_full = db.Column(db.String(250))\n",
    "    name_only = db.Column(db.String(250))\n",
    "    name_woe = db.Column(db.String(250))\n",
    "    place_type = db.Column(db.String(250))\n",
    "    state_name = db.Column(db.String(250))\n",
    "    state_name_only = db.Column(db.String(250))\n",
    "    state_woeid = db.Column(db.Integer)\n",
    "    timezone = db.Column(db.String(250))\n",
    "\n",
    "    my_trends = db.relationship('Trend', backref=db.backref('my_location', lazy=True))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"<Location {self.name_full} [updated_at: {self.updated_at}>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database schema for Twitter 'trends' table\n",
    "class Trend(db.Model):\n",
    "    __tablename__ = 'trends'\n",
    "    \n",
    "    # Defining the columns for the table 'trends',\n",
    "    # which will hold all of the top trends associated with\n",
    "    # locations in the 'locations' table\n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    updated_at = db.Column( db.DateTime )\n",
    "    woeid = db.Column(db.Integer, db.ForeignKey('locations.woeid') )\n",
    "    twitter_as_of = db.Column(db.String(100))\n",
    "    twitter_created_at = db.Column(db.String(100))\n",
    "    twitter_name = db.Column(db.String(250))\n",
    "    twitter_tweet_name = db.Column(db.String(250))\n",
    "    twitter_tweet_promoted_content = db.Column(db.String(250))\n",
    "    twitter_tweet_query = db.Column(db.String(250))\n",
    "    twitter_tweet_url = db.Column(db.String(250))\n",
    "    twitter_tweet_volume = db.Column(db.Float)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<Trend {self.my_location.name_full}: {self.twitter_tweet_name} [updated_at: {self.updated_at}>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database schema for Twitter 'trends' table\n",
    "class Tweet(db.Model):\n",
    "    __tablename__ = 'tweets'\n",
    "    \n",
    "    # Defining the columns for the table 'tweets',\n",
    "    # which will hold tweets associated the search terms in the 'trends' table,\n",
    "    # which are referred to in that table as \"twitter_tweet_name\"\n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    updated_at = db.Column( db.DateTime )\n",
    "    \n",
    "    tweet_id = db.Column( db.Integer )\n",
    "    tweet_id_str = db.Column( db.String(50), unique=True, nullable=False )\n",
    "    # tweet_search_term = db.Column(db.Integer, db.ForeignKey('trends.twitter_tweet_name') )\n",
    "    tweet_search_term = db.Column(db.String(250))\n",
    "    tweet_created_at = db.Column(db.String(100))\n",
    "   \n",
    "    tweet_is_a_quote_flag = db.Column( db.Boolean )\n",
    "    tweet_is_a_retweet_flag = db.Column( db.Boolean )\n",
    "\n",
    "    tweet_entities_hashtags_count = db.Column( db.Integer )\n",
    "    tweet_entities_user_mentions_count = db.Column( db.Integer )\n",
    "    tweet_favorite_counts = db.Column( db.Integer )\n",
    "    tweet_retweet_counts = db.Column( db.Integer )\n",
    "    \n",
    "    tweet_lang = db.Column( db.String(10) )\n",
    "    tweet_source = db.Column(db.String(250))\n",
    "    tweet_text = db.Column(db.String(250))    \n",
    "    \n",
    "    tweet_user_id = db.Column( db.Integer )\n",
    "    tweet_user_id_str = db.Column( db.String(50) )\n",
    "    tweet_user_created_at = db.Column(db.String(100))\n",
    "    tweet_user_lang = db.Column( db.String(10) )\n",
    "    tweet_user_name = db.Column( db.String(100) )\n",
    "    tweet_user_screen_name = db.Column( db.String(100) )\n",
    "    tweet_user_description = db.Column( db.String(250) )\n",
    "    tweet_user_statuses_count = db.Column( db.Integer )\n",
    "    tweet_user_favourites_count = db.Column( db.Integer )\n",
    "    tweet_user_followers_count = db.Column( db.Integer )\n",
    "    tweet_user_friends_count = db.Column( db.Integer )\n",
    "    tweet_user_listed_count = db.Column( db.Integer )\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"<Tweet {self.tweet_search_term}: {self.tweet_id} [updated_at: {self.updated_at}>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code: db_management.py - Database Update Functions\n",
    "# In app.py: `from .db_management import (`\n",
    "#     `api_rate_limits, api_calls_remaining, api_time_before_reset,`\n",
    "#     `update_db_locations_table, update_db_trends_table,`\n",
    "#     `parse_date_range`\n",
    "#     `)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project 3 - GeoTweet+\n",
    "# \n",
    "# @Author Jeffery Brown (daddyjab)\n",
    "# @Date 5/1/19\n",
    "# @File db_management.py\n",
    "\n",
    "# This file contains function which update the\n",
    "# 'tritter_trends.sqlite' database tables\n",
    "# 'locations' and 'trends' via API calls to Twitter and Flickr\n",
    "\n",
    "# The following dependencies are only required for update/mgmt of\n",
    "# 'locations' and 'trends' data\n",
    "# datetime (datetime, date) and dateutil(parser)\n",
    "# may be required by some Flask routes\n",
    "# indirectly via the parse_date_range() function\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date\n",
    "from dateutil import tz, parser\n",
    "\n",
    "import requests\n",
    "from requests.utils import quote\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Import a pointer to the Flask-SQLAlchemy database session\n",
    "# created in the main app.py file\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:         from .app import db, app\n",
    "\n",
    "# Import the Database models defined in the models.py file\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:         from .models import Location, Trend\n",
    "\n",
    "# Only perform import of local API config file if this Flask app is being run locally.\n",
    "# If being run from Heroku the keys will be provided\n",
    "# via the app environment variables configured there\n",
    "\n",
    "try:\n",
    "    # This will run if the keys are all set via Heroku environment\n",
    "\n",
    "    # Twitter API\n",
    "    key_twitter_tweetquestor_consumer_api_key = os.environ['key_twitter_tweetquestor_consumer_api_key']\n",
    "    key_twitter_tweetquestor_consumer_api_secret_key = os.environ['key_twitter_tweetquestor_consumer_api_secret_key']\n",
    "    key_twitter_tweetquestor_access_token = os.environ['key_twitter_tweetquestor_access_token']\n",
    "    key_twitter_tweetquestor_access_secret_token = os.environ['key_twitter_tweetquestor_access_secret_token']\n",
    "\n",
    "    # Flickr API\n",
    "    key_flicker_infoquestor_key = os.environ['key_flicker_infoquestor_key']\n",
    "    key_flicker_infoquestor_secret = os.environ['key_flicker_infoquestor_secret']\n",
    "\n",
    "except KeyError:\n",
    "    # Keys have not been set in the environment\n",
    "    # So need to import them locally\n",
    "    try:\n",
    "        # Twitter API keys\n",
    "        # Flickr API keys\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:         from .api_config import *\n",
    "# CODE TO RUN IN JUPYTER NOTEBOOK\n",
    "        from api_config import *\n",
    "\n",
    "    # If the api_config file is not available, then all we can do is flag an error\n",
    "    except ImportError:\n",
    "        print(\"Import Keys: At least one of the API Keys has not been populated on Heroku, and api_config not available!\")\n",
    "\n",
    "# Setup Tweepy API Authentication to access Twitter\n",
    "import tweepy\n",
    "\n",
    "try:\n",
    "    auth = tweepy.OAuthHandler(key_twitter_tweetquestor_consumer_api_key, key_twitter_tweetquestor_consumer_api_secret_key)\n",
    "    auth.set_access_token(key_twitter_tweetquestor_access_token, key_twitter_tweetquestor_access_secret_token)\n",
    "    api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())\n",
    "\n",
    "except TweepError:\n",
    "    print(\"Authentication error: Problem authenticating Twitter API using Tweepy (TweepError)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function Definitions: Twitter API Rate Limit Management\n",
    "\n",
    "def api_rate_limits():\n",
    "# Return the number of Twitter API calls remaining\n",
    "# for the specified API type:\n",
    "# \"trends/place\": Top 10 trending topics for a WOEID\n",
    "# \"trends/closest\": Locations near a specificed lat/long for which Twitter has trending topic info\n",
    "# \"trends/available\": Locations for which Twitter has topic info\n",
    "# \"search/tweets\": \n",
    "# \"users/search\"\n",
    "# \"users/shows\"\n",
    "# \"users/lookup\"\n",
    "# \n",
    "# Global Variable: 'api': Tweepy API\n",
    "# \n",
    "\n",
    "    # Get Twitter rate limit information using the Tweepy API\n",
    "    try:\n",
    "        rate_limits = api.rate_limit_status()\n",
    "    \n",
    "    except RateLimitError as e:\n",
    "        print(\"Tweepy API: Problem getting Twitter rate limits information using tweepy - RateLimitError\")\n",
    "        pprint(e)\n",
    "        \n",
    "    except:\n",
    "        print(\"Tweepy API: Problem getting Twitter rate limits information using tweepy\")\n",
    "        return \"\"\n",
    "\n",
    "    # Return the remaining requests available for the\n",
    "    # requested type of trends query (or \"\" if not a valid type)\n",
    "    try:\n",
    "        return rate_limits['resources']\n",
    "\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_calls_remaining( a_type = \"place\"):\n",
    "# Return the number of Twitter API calls remaining\n",
    "# for the specified API type:\n",
    "# 'place': Top 10 trending topics for a WOEID\n",
    "# 'closest': Locations near a specificed lat/long for which Twitter has trending topic info\n",
    "# 'available': Locations for which Twitter has topic info\n",
    "# \n",
    "# Global Variable: 'api': Tweepy API\n",
    "# \n",
    "\n",
    "    # Get Twitter rate limit information using the Tweepy API\n",
    "    rate_limits = api.rate_limit_status()\n",
    "    \n",
    "    # Focus on the rate limits for trends calls\n",
    "    trends_limits = rate_limits['resources']['trends']\n",
    "    \n",
    "    # Return the remaining requests available for the\n",
    "    # requested type of trends query (or \"\" if not a valid type)\n",
    "    try:\n",
    "        remaining = trends_limits[ f\"/trends/{a_type}\" ]['remaining']\n",
    "        print(f\"Twitter API 'trends/{a_type}' - API Calls Remaining: {remaining}\")\n",
    "\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "    return remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_time_before_reset( a_type = \"place\"):\n",
    "# Return the number of minutes until the Twitter API is reset\n",
    "# for the specified API type:\n",
    "# 'place': Top 10 trending topics for a WOEID\n",
    "# 'closest': Locations near a specificed lat/long for which Twitter has trending topic info\n",
    "# 'available': Locations for which Twitter has topic info\n",
    "# \n",
    "# Global Variable: 'api': Tweepy API\n",
    "# \n",
    "\n",
    "    # Get Twitter rate limit information using the Tweepy API\n",
    "    rate_limits = api.rate_limit_status()\n",
    "    \n",
    "    # Focus on the rate limits for trends calls\n",
    "    trends_limits = rate_limits['resources']['trends']\n",
    "    \n",
    "    \n",
    "    # Return the reset time for the\n",
    "    # requested type of trends query (or \"\" if not a valid type)\n",
    "    try:\n",
    "        reset_ts = trends_limits[ f\"/trends/{a_type}\" ]['reset']\n",
    "    except:\n",
    "        return -1\n",
    "        \n",
    "    # Calculate the remaining time using datetime methods to\n",
    "    # get the UTC time from the POSIX timestamp\n",
    "    reset_utc = datetime.utcfromtimestamp(reset_ts)\n",
    "    \n",
    "    # Current the current time\n",
    "    current_utc = datetime.utcnow()\n",
    "    \n",
    "    # Calculate the number of seconds remaining,\n",
    "    # Assumption: reset time will be >= current time\n",
    "    time_before_reset = (reset_utc - current_utc).total_seconds() / 60.0\n",
    "    \n",
    "    # Tell the datetime object that it's in UTC time zone since \n",
    "    # datetime objects are 'naive' by default\n",
    "    reset_utc = reset_utc.replace(tzinfo = tz.tzutc() )\n",
    "    \n",
    "    # Convert time zone\n",
    "    reset_local = reset_utc.astimezone( tz.tzlocal() )\n",
    "\n",
    "    # Tell the datetime object that it's in UTC time zone since \n",
    "    # datetime objects are 'naive' by default\n",
    "    current_utc = current_utc.replace(tzinfo = tz.tzutc() )\n",
    "    \n",
    "    # Convert time zone\n",
    "    current_local = current_utc.astimezone( tz.tzlocal() )\n",
    "    print(f\"Twitter API 'trends/{a_type}' - Time Before Rate Limit Reset: {time_before_reset:.1f}: Reset Time: {reset_local.strftime('%Y-%m-%d %H:%M:%S')}, Local Time: {current_local.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Return the time before reset (in minutes)\n",
    "    return time_before_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function Definitions: Twitter Locations with Available Trends Info\n",
    "\n",
    "def get_loc_with_trends_available_to_df( ):\n",
    "# Get locations that have trends data from a api.trends_available() call,\n",
    "# flatten the data, and create a dataframe\n",
    "\n",
    "    # Obtain the WOEID locations for which Twitter Trends info is available\n",
    "    try:\n",
    "        trends_avail = api.trends_available()\n",
    "        \n",
    "    except:\n",
    "        # No locations info available, return False\n",
    "        print(f\"Tweepy API: Problem getting locations that have trends available information\")\n",
    "        return False\n",
    "    \n",
    "    # Import trend availability info into a dataframe\n",
    "    trends_avail_df = pd.DataFrame.from_dict(trends_avail, orient='columns')\n",
    "    \n",
    "    # Set the 'updated_at' column to the current time in UTC timezone for all locations\n",
    "    trends_avail_df['updated_at'] = datetime.utcnow()\n",
    "\n",
    "    # Retain only locations in the U.S.\n",
    "    trends_avail_df = trends_avail_df[ (trends_avail_df['countryCode'] == \"US\") ]\n",
    "        \n",
    "    # Reset the index\n",
    "    trends_avail_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Flatten the dataframe by unpacking the placeType column information into separate columns\n",
    "    trends_avail_df['twitter_type'] = trends_avail_df['placeType'].map( lambda x: x['name'])\n",
    "\n",
    "    # Remove unneeded fields\n",
    "    trends_avail_df.drop(['placeType', 'url' ], axis='columns' , inplace = True)\n",
    "\n",
    "    # Rename the fields\n",
    "    trends_avail_df.rename(columns={\n",
    "        'woeid': 'woeid',\n",
    "        'country': 'twitter_country',\n",
    "        'countryCode': 'tritter_country_code',\n",
    "        'name': 'twitter_name',\n",
    "        'parentid': 'twitter_parentid' }, inplace=True)\n",
    "    \n",
    "    return trends_avail_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location_info( a_woeid ):\n",
    "# Use Flickr API call to get location information associated with a Yahoo! WOEID\n",
    "# Note: Yahoo! no longer supports this type of lookup! :(\n",
    "\n",
    "    # Setup the Flickr API base URL\n",
    "    flickr_api_base_url = f\"https://api.flickr.com/services/rest/?method=flickr.places.getInfo&api_key={key_flicker_infoquestor_key}&format=json&nojsoncallback=1&woe_id=\"\n",
    "\n",
    "    # Populate the WOEID and convert to string format\n",
    "    woeid_to_search = str(a_woeid)\n",
    "    \n",
    "    # Build the full URL for API REST request\n",
    "    flickr_api_url = flickr_api_base_url + woeid_to_search\n",
    "\n",
    "    try:\n",
    "        # Get the REST response, which will be in JSON format\n",
    "        response = requests.get(url=flickr_api_url)\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Flickr API: Problem getting location information for WOEID {a_woeid}: \")\n",
    "        return False\n",
    "    \n",
    "    # Parse the json\n",
    "    location_data = response.json()\n",
    "    \n",
    "    # Check for failure to locate the information\n",
    "    if (location_data['stat'] == 'fail'):\n",
    "        print(f\"Flickr API: Problem finding location WOEID {a_woeid}: {location_data['message']}\")\n",
    "        \n",
    "        \n",
    "    #pprint(location_data)\n",
    "    \n",
    "    # Return just a useful subset of the location info as flattened dictionary\n",
    "    key_location_info = {}\n",
    "    \n",
    "    # Basic information that should be present for any location\n",
    "    try:\n",
    "        key_location_info.update( {\n",
    "            'woeid': int(location_data['place']['woeid']),\n",
    "            'name_woe': location_data['place']['woe_name'],\n",
    "            'name_full': location_data['place']['name'],\n",
    "            'name_only': location_data['place']['name'].split(\",\")[0].strip(),\n",
    "            'place_type': location_data['place']['place_type'],\n",
    "            'latitude': float(location_data['place']['latitude']),\n",
    "            'longitude': float(location_data['place']['longitude']),\n",
    "        })\n",
    "                \n",
    "    except:\n",
    "        print(\"Error - basic location information not returned for WOEID{a_woeid}: \", sys.exc_info()[0])\n",
    "    \n",
    "    # Timezone associated with the location - if available\n",
    "    try:\n",
    "        key_location_info.update( {\n",
    "            'timezone': location_data['place']['timezone']  \n",
    "        })\n",
    "        \n",
    "    except:\n",
    "        key_location_info.update( {\n",
    "            'timezone': None\n",
    "        })\n",
    "        \n",
    "    # County associated with the location - if available\n",
    "    try:\n",
    "        key_location_info.update( {\n",
    "            'county_name': location_data['place']['county']['_content'],\n",
    "            'county_name_only': location_data['place']['county']['_content'].split(\",\")[0].strip(),\n",
    "            'county_woeid': int(location_data['place']['county']['woeid']),\n",
    "        })\n",
    "    except:\n",
    "        key_location_info.update( {\n",
    "            'county_name': None,\n",
    "            'county_name_only': None,\n",
    "            'county_woeid': None,\n",
    "        })\n",
    "        \n",
    "    # State associated with the location - if available\n",
    "    try:\n",
    "        key_location_info.update( {\n",
    "            'state_name': location_data['place']['region']['_content'],\n",
    "            'state_name_only': location_data['place']['region']['_content'].split(\",\")[0].strip(),\n",
    "            'state_woeid': int(location_data['place']['region']['woeid']),\n",
    "        })\n",
    "    except:\n",
    "        key_location_info.update( {\n",
    "            'state_name': None,\n",
    "            'state_name_only': None,\n",
    "            'state_woeid': None,\n",
    "        })\n",
    "        \n",
    "    # Country associated with the location - if available\n",
    "    try:\n",
    "        key_location_info.update( {\n",
    "            'country_name': location_data['place']['country']['_content'],\n",
    "            'country_name_only': location_data['place']['country']['_content'].split(\",\")[0].strip(),\n",
    "            'country_woeid': int(location_data['place']['country']['woeid']),\n",
    "        })\n",
    "    except:\n",
    "        key_location_info.update( {\n",
    "            'country_name': None,\n",
    "            'country_name_only': None,\n",
    "            'country_woeid': None, \n",
    "        })\n",
    "    \n",
    "    return key_location_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_db_locations_table():\n",
    "# Function to update the list of Twitter locations in the'locations' DB table.\n",
    "# This function uses a Twitter API to get the list of locations for which top trends\n",
    "# information is available.  It then uses a Flickr API to obtain location details for\n",
    "# each of these Twitter specified locations.  A merge is then performed of the two\n",
    "# DataFrames, resulting in a single dataframe that is used to update the 'locations' table.\n",
    "# NOTE: The Twitter 'trends/available' API call is not rate limited.\n",
    "#\n",
    "# This function assumes that the 'locations' table in the database has already been configured\n",
    "# and is ready for data.\n",
    "\n",
    "    # Flatten the Twitter Trends results and populate in a Dataframe\n",
    "    loc_with_trends_available_df = get_loc_with_trends_available_to_df( )\n",
    "\n",
    "    # Use the get_location_info() function to add location info (from Flickr)\n",
    "    # for each location (Twitter WOEID) that has trend info\n",
    "    loc_info_list =  list( loc_with_trends_available_df['woeid'].apply( get_location_info ) )\n",
    "\n",
    "    # Create a DataFrame from the location info list\n",
    "    loc_info_df = pd.DataFrame.from_dict(loc_info_list)\n",
    "\n",
    "    # Merge the Twitter trend location available dataframe with the\n",
    "    # location info dataframe to create a master list of all\n",
    "    # Twitter Trend locations and associated location information\n",
    "    twitter_trend_locations_df = loc_with_trends_available_df.merge(loc_info_df, how='inner', on='woeid')\n",
    "\n",
    "    # Delete all location information currently in the database 'locations' table\n",
    "\n",
    "    # CHANGED FOR GeoTweet+: Keep all entries - don't delete them!\n",
    "    # db.session.query(Location).delete()\n",
    "    # db.session.commit()\n",
    "\n",
    "    # Write this table of location data to the database 'locations' table\n",
    "    # twitter_trend_locations_df.to_sql( 'locations', con=db.engine, if_exists='append', index=False)\n",
    "    # db.session.commit()\n",
    "\n",
    "    # CHANGED FOR GeoTweet+: Update locations already in the table and add locations that are not\n",
    "    # There is no cross-database SQLAlchemy support for the 'upsert' operation,\n",
    "    # So query for each WOEID in the dataframe and decide if an 'add' or an 'update' is needed...\n",
    "    \n",
    "    # Convert all 'NaN' values to 'None' to avoid issues when updating the database\n",
    "    # Note: Some cities had county_woeid set to \"NaN\", which caused much havoc with db operations\n",
    "    twitter_trend_locations_df = twitter_trend_locations_df.where((pd.notnull(twitter_trend_locations_df)), None)\n",
    "    \n",
    "    # Loop through all rows in the update dataframe\n",
    "    n_adds = 0\n",
    "    n_updates = 0\n",
    "    for index, row in twitter_trend_locations_df.iterrows():\n",
    "        # Get this row into a dictionary, but exclude primary key 'woeid'\n",
    "        row_dict = row.to_dict()\n",
    "\n",
    "        # pprint(f\"DataFrame: {row['woeid']}\")\n",
    "        result = db.session.query(Location).filter( Location.woeid == row['woeid'] ).first()\n",
    "\n",
    "        if result is None:\n",
    "            # This location is not in the table, so add this entrry to the 'locations' table.\n",
    "            # NOTE: \n",
    "            # Location is the Class mapped to the 'locations' table\n",
    "            # row_dict is a dictionary containing all of the column values for this row as key/value pairs\n",
    "            # The term \"**row_dict\" creates a \"key=value\" parameter for each key/value pair\n",
    "#             print(f\"ADD: DataFrame twitter_trend_locations_df: {row['woeid']} => Database 'locations': New Entry\")\n",
    "            try:\n",
    "                db.session.add( Location(**row_dict) )\n",
    "                db.session.commit()\n",
    "                n_adds += 1\n",
    "                \n",
    "            except:\n",
    "                print(f\">>> Error while attempting to add record to 'locations'\")\n",
    "                db.session.rollback()\n",
    "            \n",
    "        else:\n",
    "            # This location is in the table, so update this entry in the 'locations' table.\n",
    "#             print(f\"UPDATE: DataFrame twitter_trend_locations_df: {row['woeid']} => Database 'locations': {result.woeid}: {result.name_full}\")\n",
    "            \n",
    "            try:\n",
    "                db.session.query(Location).filter( Location.woeid == row['woeid'] ).update( row_dict )\n",
    "                db.session.commit()\n",
    "                n_updates += 1\n",
    "                \n",
    "            except:\n",
    "                print(f\">>> Error while attempting to update record in 'locations'\")\n",
    "                db.session.rollback()\n",
    "                \n",
    "    # Return the total number of entries in the Locations table\n",
    "    num_loc = db.session.query(Location).count()\n",
    "    \n",
    "#   print(f\"Adds/Updates complete: Adds: {n_adds}, Updates {n_updates} => Rows in 'locations' table: {num_loc}\")\n",
    "    \n",
    "    return num_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function Definitions: Twitter Top Trends for Twitter Locations\n",
    "\n",
    "def get_trends_for_loc( a_woeid ):\n",
    "# Get top Twitter trending tweets for a location specified by a WOEID,\n",
    "# flatten the data, and return it as a list of dictionaries\n",
    "\n",
    "    # Import trend availability info into a dataframe\n",
    "    try:\n",
    "        top_trends = api.trends_place( a_woeid )[0]\n",
    "        \n",
    "    except:\n",
    "        # No top trends info available for this WOEID, return False\n",
    "        print(f\"Tweepy API: Problem getting trends information for WOEID {a_woeid}\")\n",
    "        return False\n",
    "    \n",
    "    #pprint(top_trends)\n",
    "    \n",
    "    # Repeat some information that is common for all elements in the trends list\n",
    "    common_info = {}\n",
    "        \n",
    "    # Basic information that should be present for any location\n",
    "    # 'updated_at': Current time in UTC timezone\n",
    "    # 'as_of': '2019-03-26T21:22:42Z',\n",
    "    # 'created_at': '2019-03-26T21:17:18Z',\n",
    "    # 'locations': [{'name': 'Atlanta', 'woeid': 2357024}]\n",
    "    try:\n",
    "        common_info.update( {\n",
    "            'woeid': int(top_trends['locations'][0]['woeid']),\n",
    "            'updated_at': datetime.utcnow(),\n",
    "            'twitter_name': top_trends['locations'][0]['name'],\n",
    "            'twitter_created_at': top_trends['created_at'],\n",
    "            'twitter_as_of': top_trends['as_of']\n",
    "        })\n",
    "                \n",
    "    except:\n",
    "        print(\"Error - basic location information not returned for WOEID{a_woeid}: \", sys.exc_info()[0])\n",
    "   \n",
    "    # Loop through all of the trends and store in an array of dictionary elements\n",
    "    # 'name': 'Jussie Smollett'\n",
    "    # 'promoted_content': None\n",
    "    # 'query': '%22Jussie+Smollett%22'\n",
    "    # 'tweet_volume': 581331\n",
    "    # 'url': 'http://twitter.com/search?q=%22Jussie+Smollett%22'\n",
    "\n",
    "    # Return the trends as an array of flattened dictionaries\n",
    "    trend_info = []\n",
    "\n",
    "    for ti in top_trends['trends']:\n",
    "        \n",
    "        # Put the trend info into a dictionary, starting with the common info\n",
    "        this_trend = common_info.copy()\n",
    "        \n",
    "        # Timezone associated with the location - if available\n",
    "        try:\n",
    "            this_trend.update( {\n",
    "                'twitter_tweet_name': ti['name'],\n",
    "                'twitter_tweet_promoted_content': ti['promoted_content'],\n",
    "                'twitter_tweet_query': ti['query'],\n",
    "                'twitter_tweet_volume': ti['tweet_volume'],\n",
    "                'twitter_tweet_url': ti['url']\n",
    "            })\n",
    "\n",
    "        except:\n",
    "            this_trend.update( {\n",
    "                'twitter_tweet_name': None,\n",
    "                'twitter_tweet_promoted_content': None,\n",
    "                'twitter_tweet_query': None,\n",
    "                'twitter_tweet_volume': None,\n",
    "                'twitter_tweet_url': None\n",
    "            })\n",
    "            \n",
    "        # Append this trend to the list\n",
    "        trend_info.append( this_trend )\n",
    "    \n",
    "    return trend_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_db_trends_table():\n",
    "# Function to obtain the list of Twitter locations from the 'locations' DB table.\n",
    "# The function then loops through each location,\n",
    "# obtains the Twitter top trends info, and then appends that data to the 'trends' table.\n",
    "# The function uses rate limit check functions to see if the Twitter API call rate limit\n",
    "# is about to be reached, and if so, delays the next relevant API call until the rate limit\n",
    "# is scheduled to be reset (a period of up to 15minutes) before continuing.\n",
    "#\n",
    "# This function assumes that the 'trends' table in the database has already been configured\n",
    "# and is ready for data.\n",
    "\n",
    "    # Obtain the list of Twitter locations from the 'locations' DB table\n",
    "    loc_list = [ x[0] for x in db.session.query(Location.woeid).all()]\n",
    "    print(f\"Retrieved {len(loc_list)} locations for processing\")\n",
    "    \n",
    "    # Keep track of the actual number of locations\n",
    "    # where trend info was written to the 'trends' table\n",
    "    num_location_trends_written_to_db = 0\n",
    "    \n",
    "    for tw_woeid in loc_list:\n",
    "        print(f\">> Updating trends for location {tw_woeid}\")\n",
    "\n",
    "        # Make sure we haven't hit the rate limit yet\n",
    "        calls_remaining = api_calls_remaining( \"place\" )\n",
    "        time_before_reset = api_time_before_reset( \"place\" )\n",
    "\n",
    "        # If we're close to hitting the rate limit for the trends/place API,\n",
    "        # then wait until the next reset =\n",
    "        # 'time_before_reset' minutes + 1 minute buffer\n",
    "        if (calls_remaining < 2):\n",
    "            print (f\">> Waiting {time_before_reset} minutes due to rate limit\")\n",
    "            time.sleep( (time_before_reset+1) * 60)\n",
    "\n",
    "        # Get trend info for a WOEID location\n",
    "        t_info = get_trends_for_loc(tw_woeid)\n",
    "\n",
    "        try:\n",
    "            # Create a DataFrame\n",
    "            t_info_df = pd.DataFrame.from_dict(t_info)\n",
    "            \n",
    "            # Delete any trends associated with this WOEID\n",
    "            # before appending new trends to the 'trends' table for this WOEID\n",
    "            \n",
    "            # CHANGED FOR GeoTweet+: Keep all entries - don't delete them!\n",
    "            # db.session.query(Trend).filter(Trend.woeid == tw_woeid).delete()\n",
    "            # db.session.commit()\n",
    "\n",
    "            # Append trends for this WOEID to the 'trends' database table\n",
    "            t_info_df.to_sql( 'trends', con=db.engine, if_exists='append', index=False)\n",
    "            db.session.commit()\n",
    "\n",
    "            # Increment the count\n",
    "            num_location_trends_written_to_db += 1\n",
    "\n",
    "        except:\n",
    "            print(f\">> Error occurred with location {tw_woeid} while attempting to prepare and write trends data\")\n",
    "            \n",
    "    return num_location_trends_written_to_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Helper Functions Supporting Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date_range(a_date_range = None):\n",
    "# Function to parse date ranges specified with the Flask API '/period' routes\n",
    "# Note, \n",
    "# Arguments: Single string a_date_range with possible formats:\n",
    "#     a_date_range = \"2019-03-01\"    ->   \">= 3/1/19\"\n",
    "#     a_date_range = \":2019-06-01\"    ->   \"<= 6/30/19\"\n",
    "#     a_date_range = \"2019-03-01:2019-06-30\"  ->   \">= 3/1/19 and  <= 6/30/19\"\n",
    "#     a_date_range = \"all\"  -> all dates\n",
    "#     a_date_range = \":\"  -> same as \"all\"\n",
    "#     a_date_range = \"\"   -> same as \"all\"\n",
    "#\n",
    "# Returns:\n",
    "#     start_date: Earliest date (inclusive), for use in date comparison\n",
    "#     end_date: Latest date (inclusive), for use in date comparison\n",
    "#     If either date cannot be parsed, an error message is returned\n",
    "\n",
    "    # Max and Min dates\n",
    "    DATE_EARLIEST_POSSIBLE = parser.parse(\"2000-01-01\").date()\n",
    "    DATE_LATEST_POSSIBLE = parser.parse(\"2100-12-31\").date()\n",
    "\n",
    "    # Initialize default return valus - no date restriction\n",
    "    start_date = DATE_EARLIEST_POSSIBLE\n",
    "    end_date = DATE_LATEST_POSSIBLE\n",
    "    \n",
    "    # Parse the argument to obtain the start and end dates - if provided\n",
    "    \n",
    "    # If no argument provided, provide full date range (i.e., no date restriction)\n",
    "    if a_date_range is None:\n",
    "        # Return default values\n",
    "        return (start_date, end_date)\n",
    "\n",
    "    # Prep the date range for additional processing\n",
    "    date_range = a_date_range.strip().lower()\n",
    "    \n",
    "    # Check for \"all\" and similar indications of no date restriction\n",
    "    if date_range == \"all\" or date_range == \"\" or date_range == \":\" :\n",
    "        # Return default values\n",
    "        return (start_date, end_date)\n",
    "    \n",
    "    # Attempt to split the date range (seperator = \":\")\n",
    "    arg_list = a_date_range.split(\":\")\n",
    "    \n",
    "    # If only one argument provided (i.e., no \":\")\n",
    "    # then restrict date range to just that one date\n",
    "    if len(arg_list) == 1:\n",
    "        try:\n",
    "            start_date = parser.parse(arg_list[0]).date()\n",
    "            end_date = start_date\n",
    "            \n",
    "        except ValueError:\n",
    "            start_date = f\"ERROR\"\n",
    "            end_date = start_date\n",
    "\n",
    "        return (start_date, end_date)\n",
    "    \n",
    "    # At least 2 args provided, so assume they are start and end dates\n",
    "    \n",
    "    # Populate start date if the argument is populated, otherwise leave the default\n",
    "    if len(arg_list[0])>0:\n",
    "        try:\n",
    "            start_date = parser.parse(arg_list[0]).date()\n",
    "        except ValueError:\n",
    "            start_date = f\"ERROR\"\n",
    "\n",
    "    # Populate end date if the argument is populated, otherwise leave the default\n",
    "    if len(arg_list[1])>0:\n",
    "        try:\n",
    "            end_date = parser.parse(arg_list[1]).date()\n",
    "        except ValueError:\n",
    "            end_date =  f\"ERROR\"\n",
    "\n",
    "    # Get the date range from the arguments\n",
    "    return (start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB Management: Twitter Tweet info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function Definitions: Twitter Tweet Info\n",
    "def search_for_tweets( a_search_term ):\n",
    "# Get a list of specific tweets associated with search term a_search_term,\n",
    "# flatten the relevant data, and return it as a list of dictionaries\n",
    "\n",
    "    # Number of tweets per page (up to 100) to be returned from the API query\n",
    "    tweets_count_limit = 100       # PRODUCTION\n",
    "    # tweets_count_limit = 5         # DEBUG\n",
    "    \n",
    "    try:\n",
    "        # Perform API search query and obtain only the 1st page of results\n",
    "        tweets = api.search(quote(a_search_term), lang='en', count=tweets_count_limit)\n",
    "        \n",
    "    except:\n",
    "        # No tweet info available for this search term, return False\n",
    "        print(f\"Tweepy API Error: Problem getting tweet information for search term {a_search_term}\")\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    # Create a list of dictionaries of Tweets info associated with a_search_term\n",
    "    tweet_list = []\n",
    "\n",
    "    # Repeat some information that is common for all elements in the tweet list\n",
    "    common_info = {\n",
    "        'updated_at': datetime.utcnow(),\n",
    "        'tweet_search_term': a_search_term\n",
    "    }\n",
    "\n",
    "    # Loop through each tweet in the tweet search results\n",
    "    for t in tweets['statuses']:\n",
    "        \n",
    "        # Start the dictionary with some common information\n",
    "        tweet_info = dict(common_info)\n",
    "\n",
    "        # Info about this Tweet (i.e., \"Status\")\n",
    "        try:            \n",
    "            tweet_info.update( {                \n",
    "                'tweet_id': t['id'],\n",
    "                'tweet_id_str': t['id_str'],\n",
    "                'tweet_created_at': t['created_at'],\n",
    "                'tweet_text': t['text'],\n",
    "                'tweet_lang': t['lang'],\n",
    "                'tweet_source': t['source'],\n",
    "                'tweet_is_a_quote_flag': t['is_quote_status'],    # If True, then this is a Quoted Tweet (i.e., Tweet w/ comments/mods)\n",
    "            })\n",
    "\n",
    "        except:\n",
    "            print(f\"Tweepy API Error: Problem getting tweet-related info\")\n",
    "\n",
    "        # If the 'retweeted_status' key exists in the results,\n",
    "        # then this Tweet is a Retweet (i.e., Tweet forwarded \"as is\")\n",
    "        if 'retweeted_status' in t:\n",
    "            tweet_info.update( { 'tweet_is_a_retweet_flag': True })\n",
    "        else:\n",
    "            tweet_info.update( { 'tweet_is_a_retweet_flag': False })\n",
    "\n",
    "        # Counts associated with the tweet\n",
    "        try:            \n",
    "            tweet_info.update( {                \n",
    "                'tweet_entities_hashtags_count': len(t['entities']['hashtags']),\n",
    "                'tweet_entities_user_mentions_count': len(t['entities']['user_mentions']),\n",
    "                'tweet_favorite_counts': t['favorite_count'],\n",
    "                'tweet_retweet_counts': t['retweet_count'],\n",
    "            })\n",
    "\n",
    "        except:\n",
    "            print(f\"Tweepy API Error: Problem getting tweet-related info\")\n",
    "        \n",
    "        # User who created this Tweet\n",
    "        try:\n",
    "            tweet_info.update( {                \n",
    "                'tweet_user_id': t['user']['id'],\n",
    "                'tweet_user_id_str': t['user']['id_str'],\n",
    "                'tweet_user_created_at': t['user']['created_at'],\n",
    "                'tweet_user_name': t['user']['name'],\n",
    "                'tweet_user_screen_name': t['user']['screen_name'],\n",
    "                'tweet_user_description': t['user']['description'],\n",
    "                'tweet_user_lang': t['user']['lang'],\n",
    "                'tweet_user_statuses_count': t['user']['statuses_count'],     # No. of Tweets/Retweets issued by this user\n",
    "                'tweet_user_favourites_count': t['user']['favourites_count'],    # No. of Tweets this user has liked (in account's lifetime)\n",
    "                'tweet_user_followers_count': t['user']['followers_count'],     # No. of Followers this account currently has\n",
    "                'tweet_user_friends_count': t['user']['friends_count'],       # No. of Users this account is following\n",
    "                'tweet_user_listed_count': t['user']['listed_count']        # No. of Public lists this user is a member of\n",
    "            })\n",
    "            \n",
    "        except:\n",
    "            print(f\"Tweepy API Error: Problem getting user-related info\")            \n",
    "\n",
    "        # Append this tweet to the list\n",
    "        tweet_list.append( tweet_info )\n",
    "        \n",
    "        # DEBUG *******************************************************************\n",
    "        # print(f\">>> In search_for_tweets( '{a_search_term}' ) - Just appended tweet_info:\")\n",
    "        # pprint(tweet_info)\n",
    "        \n",
    "        # print(f\">>> In search_for_tweets( '{a_search_term}' ) - tweet_list is now:\")\n",
    "        # pprint(tweet_list)\n",
    "\n",
    "    \n",
    "    return(tweet_list)\n",
    "        \n",
    "#     return trend_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_terms_from_trends(a_date_range=None):\n",
    "# Get a list of the unique tweet search terms specified in\n",
    "# the 'trends' table.\n",
    "# Ensure that all tweets in the list are unique by using a Python \"set\"\n",
    "    \n",
    "    # Parse the date range\n",
    "    q_start_date, q_end_date = parse_date_range(a_date_range)\n",
    "\n",
    "    # Return with an error if there was a problem parsing the date range\n",
    "    if q_start_date == \"ERROR\" or q_end_date == \"ERROR\":\n",
    "        search_term_list = [{'ERROR': 'ERROR'}]\n",
    "        # return jsonify(search_term_list)\n",
    "        return(search_term_list)\n",
    "    \n",
    "    # Query to get the search_terms (i.e., 'twitter_tweet_name')\n",
    "    # from the 'trends' table for the specified date range\n",
    "    results = db.session.query(Trend.twitter_tweet_name) \\\n",
    "                .filter( and_( \\\n",
    "                        func.date(Trend.updated_at) >= q_start_date, \\\n",
    "                        func.date(Trend.updated_at) <= q_end_date \\\n",
    "                       )) \\\n",
    "                .order_by( Trend.twitter_tweet_name ).all()\n",
    "\n",
    "    # Get the list of unique search terms using set()\n",
    "    # Note: The results list is a list of tuples, with first tuple being the desired value\n",
    "    search_term_set = set([ t[0] for t in results])\n",
    "\n",
    "    # To support the hashtag/no hashtag Tweet Analysis,\n",
    "    # add the complementary tweet to the table for each unique tweet\n",
    "    search_term_alt_set = set([ f\"{y[1:]}\" if y[:1] == \"#\" else f\"#{y}\" for y in search_term_set ])\n",
    "\n",
    "    # Combined the sets\n",
    "    search_term_set.update(search_term_alt_set)\n",
    "    \n",
    "    # Return a list\n",
    "    search_term_list = sorted(list(search_term_set))\n",
    "\n",
    "    #     return jsonify(search_term_list)\n",
    "    return(search_term_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_terms_from_tweets(a_date_range=None):\n",
    "# Get a list of the unique tweet search terms specified in\n",
    "# the 'tweets' table.\n",
    "# Ensure that all tweets in the list are unique by using a Python \"set\"\n",
    "    \n",
    "    # Parse the date range\n",
    "    q_start_date, q_end_date = parse_date_range(a_date_range)\n",
    "\n",
    "    # Return with an error if there was a problem parsing the date range\n",
    "    if q_start_date == \"ERROR\" or q_end_date == \"ERROR\":\n",
    "        search_term_list = [{'ERROR': 'ERROR'}]\n",
    "        # return jsonify(search_term_list)\n",
    "        return(search_term_list)\n",
    "    \n",
    "    # Query to get the search_terms (i.e., 'twitter_tweet_name')\n",
    "    # from the 'tweets' table for the specified date range\n",
    "    results = db.session.query(Tweet.tweet_search_term) \\\n",
    "                .filter( and_( \\\n",
    "                        func.date(Tweet.updated_at) >= q_start_date, \\\n",
    "                        func.date(Tweet.updated_at) <= q_end_date \\\n",
    "                       )) \\\n",
    "                .order_by( Tweet.tweet_search_term ).all()\n",
    "\n",
    "    # Get the list of unique search terms using set()\n",
    "    # Note: The results list is a list of tuples, with first tuple being the desired value\n",
    "    search_term_set = set([ t[0] for t in results])\n",
    "\n",
    "    # To support the hashtag/no hashtag Tweet Analysis,\n",
    "    # add the complementary tweet to the table for each unique tweet\n",
    "    search_term_alt_set = set([ f\"{y[1:]}\" if y[:1] == \"#\" else f\"#{y}\" for y in search_term_set ])\n",
    "\n",
    "    # Combined the sets\n",
    "    search_term_set.update(search_term_alt_set)\n",
    "    \n",
    "    # Return a list\n",
    "    search_term_list = sorted(list(search_term_set))\n",
    "\n",
    "    #     return jsonify(search_term_list)\n",
    "    return(search_term_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_list():\n",
    "# Based upon the search terms in 'trends' and 'tweets' tables,\n",
    "# use the Twitter Search API to get tweets for search terms\n",
    "# that are in the 'trends' table but not already in the 'tweet' table\n",
    "    \n",
    "    # Get all of the Twitter search terms in the 'trends' table\n",
    "    trends_search_term_list = get_search_terms_from_trends()\n",
    "    \n",
    "    # Get the Twitter search terms from the 'tweets' table\n",
    "    # and remove existing search terms from the list of search terms\n",
    "    # for which api calls will be performed --> Minimizes API calls\n",
    "    tweets_search_term_list = get_search_terms_from_tweets()\n",
    "    \n",
    "    # Create a list of search terms that include all terms from the 'trends'\n",
    "    # table and removes all those already in the 'tweets' table\n",
    "    add_search_term_list = list( set(trends_search_term_list) - set(tweets_search_term_list) )\n",
    "    print( f\"Search Terms - Trends: {len(trends_search_term_list)}, Tweets {len(tweets_search_term_list)}, Add: {len(add_search_term_list)}\" )        \n",
    "    \n",
    "    #DEBUG *******************************************************************************************\n",
    "    # return add_search_term_list\n",
    "    #DEBUG *******************************************************************************************\n",
    "\n",
    "    # Loop through each search term and perform\n",
    "    # a search for tweets associated with that term\n",
    "    tweet_list = []\n",
    "    search_term_count = 0\n",
    "    \n",
    "    for s in add_search_term_list:\n",
    "        \n",
    "        # Check the rate limits to see if there's enough left to make a search\n",
    "        try:\n",
    "            retval = api_rate_limits()\n",
    "            searches_remaining = retval['search']['/search/tweets']['remaining']\n",
    "    \n",
    "        except:\n",
    "            # Most likely hit rate limits -- break out of the loop and process what we have so far\n",
    "            print(\"POSSIBLE RATE LIMITS: search tweets 'remaining' not populated in API results\")\n",
    "            break\n",
    "            \n",
    "        # If searches remaining are too low -- break out of the loop and process what we have so far\n",
    "        if searches_remaining < 10:\n",
    "            print(\"RATE LIMITS: Too close to rate limits to perform additional searches\")\n",
    "            break\n",
    "                \n",
    "        # Get Tweets for this Twitter search term\n",
    "        tweets_for_this_search_term = search_for_tweets(s)\n",
    "        print(f\"Search Term '{s}' => Tweet Count: {len(tweets_for_this_search_term)}\")\n",
    "        \n",
    "        # Build a list of Tweets\n",
    "        tweet_list.extend( tweets_for_this_search_term )\n",
    "        \n",
    "        search_term_count += 1\n",
    "        \n",
    "        # DEBUG *******************************************************************************\n",
    "        # if search_term_count > 10:\n",
    "        #    break\n",
    "        # DEBUG *******************************************************************************\n",
    "    \n",
    "    print(f\"OVERALL => Tweet Count: {len(tweet_list)}, API Search Calls: {search_term_count}\")\n",
    "    \n",
    "    # Return the tweet_list - for debugging purposes\n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_db_tweets_table(a_tweet_list):\n",
    "# Update the tweets table by adding tweets for each\n",
    "# twitter search term in the 'trends' table\n",
    "#\n",
    "# Arguments:\n",
    "#    a_tweet_list: A list of tweets generated by get_tweet_list()\n",
    "#                  to be added to the 'tweets' table\n",
    "       \n",
    "    print(f\"Tweets to add to the 'tweets' table: {len(a_tweet_list)}\")\n",
    "    \n",
    "#     try:\n",
    "#         # Create a DataFrame\n",
    "#         tweet_df = pd.DataFrame.from_dict(tweet_list)\n",
    "\n",
    "#         # Append tweets the 'trends' database table\n",
    "#         tweet_df.to_sql( 'tweets', con=db.engine, if_exists='append', index=False)\n",
    "#         db.session.commit()\n",
    "\n",
    "#         # Increment the count\n",
    "#         print(f\"Wrote {len(tweet_list)} tweets to the 'Tweets' table\")\n",
    "        \n",
    "#     except:\n",
    "#         print(f\">> Error occurred while attempting to  write tweets data\")\n",
    "        \n",
    "    # Return the total number of entries in the Locations table\n",
    "    num_tweets_start = db.session.query(Tweet).count()\n",
    "\n",
    "    # Loop through all tweet entries\n",
    "    n_adds = 0\n",
    "    n_error_adds = 0\n",
    "    n_updates = 0\n",
    "    n_error_updates = 0\n",
    "    for t in a_tweet_list:\n",
    "        \n",
    "        # Search for this tweet in the 'tweets' table -- just in case it's there\n",
    "        result = db.session.query(Tweet).filter( Tweet.tweet_id_str == t['tweet_id_str'] ).first()\n",
    "\n",
    "        if result is None:\n",
    "            # This tweet is not in the table, so add this entrry to the 'tweets' table.\n",
    "            # NOTE: \n",
    "            # Tweet is the Class mapped to the 'tweet' table\n",
    "            # t is a dictionary containing all of the column values for this row as key/value pairs\n",
    "            # The term \"**t\" creates a \"key=value\" parameter for each key/value pair\n",
    "            try:\n",
    "                db.session.add( Tweet(**t) )\n",
    "                db.session.commit()\n",
    "                n_adds += 1\n",
    "                print(f\">>> ADDED: Record to 'tweets': Search Term '{t['tweet_search_term']}' => Tweet ID: '{t['tweet_id_str']}'\")\n",
    "                \n",
    "            except:\n",
    "                n_error_adds += 1\n",
    "                print(f\">>> ADD: Error while attempting to add record to 'tweets': Search Term '{t['tweet_search_term']}' => Tweet ID: '{t['tweet_id_str']}'\")\n",
    "                db.session.rollback()\n",
    "            \n",
    "        else:\n",
    "            # DEBUG *************************************************************************************\n",
    "            # print(result)\n",
    "            # DEBUG *************************************************************************************\n",
    "            \n",
    "            # This tweet is in the table, so update this entry in the 'tweets' table.            \n",
    "            try:\n",
    "                db.session.query(Tweet).filter( Tweet.tweet_id_str == t['tweet_id_str'] ).update( t )\n",
    "                db.session.commit()\n",
    "                n_updates += 1\n",
    "                print(f\">>> UPDATED: Record in 'tweets': Search Term '{t['tweet_search_term']}' => Tweet ID: '{t['tweet_id_str']}'\")\n",
    "                \n",
    "            except:\n",
    "                n_error_updates += 1\n",
    "                print(f\">>> UPDATE: Error while attempting to add record to 'tweets': Search Term '{t['tweet_search_term']}' => Tweet ID: '{t['tweet_id_str']}'\")\n",
    "                db.session.rollback()\n",
    "                \n",
    "    # Return the total number of entries in the Locations table\n",
    "    num_tweets_finish = db.session.query(Tweet).count()\n",
    "    \n",
    "    print(f\"COMPLETE: ADDS: [{n_adds} success, {n_error_adds} error], UPDATES: [{n_updates} success, {n_error_updates}] error => 'tweets' table rows: {num_tweets_start}->{num_tweets_finish}\")\n",
    "    \n",
    "    retval = {\n",
    "        'n_tweet_list_input': len(a_tweet_list),\n",
    "        'n_tweet_table_entries_start': num_tweets_start,\n",
    "        'n_tweet_table_entries_finish': num_tweets_finish,\n",
    "        \n",
    "        'n_adds': n_adds,\n",
    "        'n_error_adds': n_error_adds,\n",
    "        'n_updates': n_updates,\n",
    "        'n_error_updates': n_error_updates\n",
    "    }\n",
    "    \n",
    "    # Return the counts of add/update actions\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(api_calls_remaining(\"available\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_loc = update_db_locations_table()\n",
    "# print(n_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(api_calls_remaining(\"place\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n_trends = update_db_trends_table()\n",
    "# print(n_trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(api_calls_remaining(\"place\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api.rate_limit_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(api_rate_limits() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'limit': 180, 'remaining': 9, 'reset': 1557170982}\n"
     ]
    }
   ],
   "source": [
    "pprint(api_rate_limits()['search']['/search/tweets'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Terms - Trends: 874, Tweets 340, Add: 534\n",
      "Search Term '#Jordi Alba' => Tweet Count: 0\n",
      "Search Term '#Lap 40' => Tweet Count: 0\n",
      "Search Term '#Preciate' => Tweet Count: 0\n",
      "Search Term '#Jake Delhomme' => Tweet Count: 0\n",
      "Search Term '#ProudToBeLBUSD' => Tweet Count: 100\n",
      "Search Term '#Hunter Renfroe' => Tweet Count: 0\n",
      "Search Term '#Paul Mainieri' => Tweet Count: 0\n",
      "Search Term '#Dawnta Harris' => Tweet Count: 0\n",
      "Search Term 'BPTWIN' => Tweet Count: 100\n",
      "Search Term '#Ben Sasse' => Tweet Count: 0\n",
      "Search Term '#dawnta harris' => Tweet Count: 0\n",
      "Search Term 'BREAKING' => Tweet Count: 83\n",
      "Search Term 'Anfield' => Tweet Count: 87\n",
      "Search Term '#MentalHealthMonth' => Tweet Count: 100\n",
      "Search Term 'Mazie' => Tweet Count: 52\n",
      "Search Term 'iPhone' => Tweet Count: 70\n",
      "Search Term '#Preakness' => Tweet Count: 99\n",
      "Search Term '#Mazie' => Tweet Count: 17\n",
      "Search Term '#ThisIsMay' => Tweet Count: 100\n",
      "Search Term 'lsugivingday' => Tweet Count: 100\n",
      "Search Term 'Lap 40' => Tweet Count: 100\n",
      "Search Term 'México' => Tweet Count: 71\n",
      "Search Term '#TXST' => Tweet Count: 100\n",
      "Search Term '#Country House' => Tweet Count: 81\n",
      "Search Term 'Jonas Brothers' => Tweet Count: 80\n",
      "Search Term 'John Singleton' => Tweet Count: 95\n",
      "Search Term 'FarFromHome' => Tweet Count: 86\n",
      "Search Term '#doesnthelooktired' => Tweet Count: 0\n",
      "Search Term '#Ted Bundy' => Tweet Count: 19\n",
      "Search Term 'EWA19' => Tweet Count: 100\n",
      "Search Term 'SDSummit' => Tweet Count: 100\n",
      "Search Term 'Ter Stegen' => Tweet Count: 90\n",
      "Search Term 'royalbaby' => Tweet Count: 90\n",
      "Search Term 'Lincoln' => Tweet Count: 80\n",
      "Search Term 'Jordi Alba' => Tweet Count: 80\n",
      "Search Term '#MusicMonday' => Tweet Count: 100\n",
      "Search Term '#BarrCoverUp' => Tweet Count: 73\n",
      "Search Term 'doesnthelooktired' => Tweet Count: 0\n",
      "Search Term 'Barca' => Tweet Count: 93\n",
      "Search Term 'YouWontHearShatnerSay' => Tweet Count: 91\n",
      "Search Term '#UNC Charlotte' => Tweet Count: 94\n",
      "Search Term '#Sonic' => Tweet Count: 79\n",
      "Search Term 'Meghan Markle' => Tweet Count: 94\n",
      "Search Term 'Facts' => Tweet Count: 90\n",
      "Search Term 'BarrResign' => Tweet Count: 96\n",
      "Search Term '#collegedecisionday' => Tweet Count: 82\n",
      "Search Term 'AG Barr' => Tweet Count: 76\n",
      "Search Term 'Justified' => Tweet Count: 79\n",
      "Search Term 'RedForEd' => Tweet Count: 100\n",
      "Search Term 'ousafe' => Tweet Count: 100\n",
      "Search Term '#BARLIV' => Tweet Count: 17\n",
      "Search Term '#Bill Barr' => Tweet Count: 22\n",
      "Search Term 'txst' => Tweet Count: 100\n",
      "Search Term '#NationalNursesWeek' => Tweet Count: 100\n",
      "Search Term 'SFGiants' => Tweet Count: 100\n",
      "Search Term 'Wilmer Font' => Tweet Count: 100\n",
      "Search Term '#ICLR2019' => Tweet Count: 100\n",
      "Search Term 'Kyrie' => Tweet Count: 91\n",
      "Search Term '#Heck' => Tweet Count: 41\n",
      "Search Term '#tnleg' => Tweet Count: 100\n",
      "Search Term '#Mother's Day' => Tweet Count: 75\n",
      "Search Term 'Yankees' => Tweet Count: 95\n",
      "Search Term 'fromMEtoYOU' => Tweet Count: 79\n",
      "Search Term '#MentalHealthAwarenessMonth' => Tweet Count: 100\n",
      "Search Term 'The Americans' => Tweet Count: 67\n",
      "Search Term '#Chase Utley' => Tweet Count: 0\n",
      "Search Term 'The Wire' => Tweet Count: 88\n",
      "Search Term '#Middleton' => Tweet Count: 99\n",
      "Search Term 'Stockton' => Tweet Count: 92\n",
      "Search Term '#México' => Tweet Count: 69\n",
      "Search Term '#William Barr' => Tweet Count: 79\n",
      "Search Term '#NationalNursesDay' => Tweet Count: 100\n",
      "Search Term 'Star Wars' => Tweet Count: 81\n",
      "Search Term 'GoBows' => Tweet Count: 100\n",
      "Search Term 'Sansa' => Tweet Count: 65\n",
      "Search Term '#FearTheDeer' => Tweet Count: 94\n",
      "Search Term 'Preciate' => Tweet Count: 98\n",
      "Search Term '#David Tepper' => Tweet Count: 0\n",
      "Search Term 'Ole Miss' => Tweet Count: 100\n",
      "Search Term 'GoogleDoodle' => Tweet Count: 100\n",
      "Search Term '#Gila2019' => Tweet Count: 100\n",
      "Search Term 'CincoDeMayo' => Tweet Count: 58\n",
      "Search Term '#Indians' => Tweet Count: 100\n",
      "Search Term '#Honestly' => Tweet Count: 93\n",
      "Search Term 'Powell' => Tweet Count: 78\n",
      "Search Term 'ARMYSelcaDay' => Tweet Count: 68\n",
      "Search Term 'UNCC' => Tweet Count: 96\n",
      "Search Term '#InternationalWorkersDay' => Tweet Count: 98\n",
      "Search Term 'MusicMonday' => Tweet Count: 100\n",
      "Search Term '#Akron' => Tweet Count: 92\n",
      "Search Term 'Preakness' => Tweet Count: 100\n",
      "Search Term 'UTEP' => Tweet Count: 100\n",
      "Search Term '#Missandei' => Tweet Count: 88\n",
      "Search Term 'Firmino' => Tweet Count: 79\n",
      "Search Term '#OperacionLibertad' => Tweet Count: 13\n",
      "Search Term 'Harry and Meghan' => Tweet Count: 94\n",
      "Search Term 'Sooners' => Tweet Count: 100\n",
      "Search Term '#St. Joseph's Catholic Church' => Tweet Count: 0\n",
      "Search Term 'Ruth Asawa' => Tweet Count: 94\n",
      "Search Term '#Jokic' => Tweet Count: 100\n",
      "Search Term '#DubNation' => Tweet Count: 100\n",
      "Search Term 'David Price' => Tweet Count: 81\n",
      "Search Term '#Justified' => Tweet Count: 88\n",
      "Search Term 'Fox News' => Tweet Count: 69\n",
      "Search Term '#bsmf19' => Tweet Count: 86\n",
      "Search Term 'GiveNOLA' => Tweet Count: 100\n",
      "Search Term '#Michelle Obama' => Tweet Count: 2\n",
      "Search Term 'Absolutely' => Tweet Count: 86\n",
      "Search Term '#Tyrion' => Tweet Count: 84\n",
      "Search Term '#news 8' => Tweet Count: 84\n",
      "Search Term 'BBMAsTopSocial' => Tweet Count: 65\n",
      "Search Term '#Birmingham' => Tweet Count: 80\n",
      "Search Term 'Ellis Parlier' => Tweet Count: 90\n",
      "Search Term 'Hunter Renfroe' => Tweet Count: 100\n",
      "Search Term 'The Shield' => Tweet Count: 58\n",
      "Search Term 'CaneloJacobs' => Tweet Count: 73\n",
      "Search Term 'Friday Night Lights' => Tweet Count: 100\n",
      "Search Term '#Cuba' => Tweet Count: 77\n",
      "Search Term '#Edmond' => Tweet Count: 97\n",
      "Search Term 'Jake Delhomme' => Tweet Count: 100\n",
      "Search Term '#Shut' => Tweet Count: 35\n",
      "Search Term '#CollegeSigningDay' => Tweet Count: 92\n",
      "Search Term 'Honestly' => Tweet Count: 80\n",
      "Search Term 'ThisIsMay' => Tweet Count: 96\n",
      "Search Term '#Mysterio' => Tweet Count: 76\n",
      "Search Term 'hitraffic' => Tweet Count: 100\n",
      "Search Term '#Jonas Brothers' => Tweet Count: 56\n",
      "Search Term 'Amen' => Tweet Count: 96\n",
      "Search Term 'Kentucky Derby' => Tweet Count: 97\n",
      "Search Term 'Somerville' => Tweet Count: 100\n",
      "Search Term 'MentalHealthMonth' => Tweet Count: 100\n",
      "Search Term 'Maui' => Tweet Count: 83\n",
      "Search Term '#BBMAsTopSocial' => Tweet Count: 66\n",
      "Search Term 'España' => Tweet Count: 86\n",
      "Search Term 'MSBuild' => Tweet Count: 100\n",
      "Search Term 'Mahalo' => Tweet Count: 100\n",
      "Search Term 'Liverpool' => Tweet Count: 88\n",
      "Search Term '#Ball Is Life' => Tweet Count: 5\n",
      "Search Term '#Mueller' => Tweet Count: 85\n",
      "Search Term 'Ball Is Life' => Tweet Count: 89\n",
      "Search Term '#YouWontHearShatnerSay' => Tweet Count: 91\n",
      "Search Term 'Biz Markie' => Tweet Count: 92\n",
      "Search Term 'Thor' => Tweet Count: 45\n",
      "Search Term '#Met Gala' => Tweet Count: 38\n",
      "Search Term '#PackingTips' => Tweet Count: 100\n",
      "Search Term '#Meghan Markle' => Tweet Count: 68\n",
      "Search Term '#GameOfThornes' => Tweet Count: 100\n",
      "Search Term 'StarbucksCup' => Tweet Count: 96\n",
      "Search Term '#Derby' => Tweet Count: 100\n",
      "Search Term '#TheActFinale' => Tweet Count: 80\n",
      "Search Term 'Harden' => Tweet Count: 87\n",
      "Search Term 'PackingTips' => Tweet Count: 100\n",
      "Search Term 'TeamDCS' => Tweet Count: 100\n",
      "Search Term '#Emily Houpt' => Tweet Count: 0\n",
      "Search Term 'OperacionLibertad' => Tweet Count: 13\n",
      "Search Term 'Kindle' => Tweet Count: 55\n",
      "Search Term '#Maximum Security' => Tweet Count: 35\n",
      "Search Term '#Ter Stegen' => Tweet Count: 6\n",
      "Search Term 'LBCC' => Tweet Count: 98\n",
      "Search Term '#BREAKING' => Tweet Count: 85\n",
      "Search Term '#Ellis Parlier' => Tweet Count: 0\n",
      "Search Term '#ThankATeacher' => Tweet Count: 100\n",
      "Search Term '#Hall of Honor' => Tweet Count: 1\n",
      "Search Term '#The Wire' => Tweet Count: 6\n",
      "Search Term 'Indians' => Tweet Count: 77\n",
      "Search Term '#Canada' => Tweet Count: 77\n",
      "Search Term '#People' => Tweet Count: 82\n",
      "Search Term 'Met Gala' => Tweet Count: 67\n",
      "Search Term '#DemThrones' => Tweet Count: 82\n",
      "Search Term '#ImpeachBarrAndtRump' => Tweet Count: 86\n",
      "Search Term '#expediachat' => Tweet Count: 40\n",
      "RATE LIMITS: Too close to rate limits to perform additional searches\n",
      "OVERALL => Tweet Count: 12587, API Search Calls: 171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12587"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_list = get_tweet_list()\n",
    "len(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets to add to the 'tweets' table: 12587\n",
      ">>> ADDED: Record to 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125477322730065920'\n",
      ">>> ADDED: Record to 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125474098677608448'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125465778847805440'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125464713293262848'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125456650570067968'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125455680033288192'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125455134706679808'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125453290039197696'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125452851763863552'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125451087849222144'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125451045180547074'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125447341580869633'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125430874646781957'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125429437321768960'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125419404492136448'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125419347143299072'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125419321419681792'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125411844816924675'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125411042161451009'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125409746373079041'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125408754927693824'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125407891522031616'\n",
      ">>> UPDATED: Record in 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125403593371033600'\n",
      ">>> UPDATE: Error while attempting to add record to 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125403284741578752'\n",
      ">>> UPDATE: Error while attempting to add record to 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125399993173532672'\n",
      ">>> UPDATE: Error while attempting to add record to 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125394389696520193'\n",
      ">>> UPDATE: Error while attempting to add record to 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125393744767766539'\n",
      ">>> UPDATE: Error while attempting to add record to 'tweets': Search Term '#ProudToBeLBUSD' => Tweet ID: '1125392681247444994'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-20-2b2c25f4e25b>\", line 62, in update_db_tweets_table\n",
      "    db.session.query(Tweet).filter( Tweet.tweet_id_str == t['tweet_id_str'] ).update( t )\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\query.py\", line 3479, in update\n",
      "    update_op.exec_()\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\persistence.py\", line 1325, in exec_\n",
      "    self._do_pre_synchronize()\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\persistence.py\", line 1407, in _do_pre_synchronize\n",
      "    query.session.identity_map.items()\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\persistence.py\", line 1409, in <listcomp>\n",
      "    eval_condition(obj)]\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\evaluator.py\", line 121, in evaluate\n",
      "    left_val = eval_left(obj)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\evaluator.py\", line 78, in <lambda>\n",
      "    return lambda obj: get_corresponding_attr(obj)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\attributes.py\", line 242, in __get__\n",
      "    return self.impl.get(instance_state(instance), dict_)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\attributes.py\", line 594, in get\n",
      "    value = state._load_expired(state, passive)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\state.py\", line 608, in _load_expired\n",
      "    self.manager.deferred_scalar_loader(self, toload)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\loading.py\", line 876, in load_scalar_attributes\n",
      "    only_load_props=attribute_names)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\loading.py\", line 188, in load_on_ident\n",
      "    identity_token=identity_token\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\loading.py\", line 250, in load_on_pk_identity\n",
      "    return q.one()\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\query.py\", line 2947, in one\n",
      "    ret = self.one_or_none()\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\query.py\", line 2917, in one_or_none\n",
      "    ret = list(self)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\loading.py\", line 75, in instances\n",
      "    fetch = cursor.fetchall()\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\engine\\result.py\", line 1137, in fetchall\n",
      "    self.cursor, self.context)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 1416, in _handle_dbapi_exception\n",
      "    util.reraise(*exc_info)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\util\\compat.py\", line 249, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\engine\\result.py\", line 1131, in fetchall\n",
      "    l = self.process_rows(self._fetchall_impl())\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\engine\\result.py\", line 1082, in _fetchall_impl\n",
      "    return self.cursor.fetchall()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-40-745653938250>\", line 1, in <module>\n",
      "    update_status = update_db_tweets_table(tweet_list)\n",
      "  File \"<ipython-input-20-2b2c25f4e25b>\", line 70, in update_db_tweets_table\n",
      "    db.session.rollback()\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\scoping.py\", line 153, in do\n",
      "    return getattr(self.registry(), name)(*args, **kwargs)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\session.py\", line 907, in rollback\n",
      "    self.transaction.rollback()\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\session.py\", line 507, in rollback\n",
      "    dirty_only=transaction.nested)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\session.py\", line 364, in _restore_snapshot\n",
      "    s._expire(s.dict, self.session.identity_map._modified)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-20-2b2c25f4e25b>\", line 62, in update_db_tweets_table\n",
      "    db.session.query(Tweet).filter( Tweet.tweet_id_str == t['tweet_id_str'] ).update( t )\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\query.py\", line 3479, in update\n",
      "    update_op.exec_()\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\persistence.py\", line 1325, in exec_\n",
      "    self._do_pre_synchronize()\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\persistence.py\", line 1407, in _do_pre_synchronize\n",
      "    query.session.identity_map.items()\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\persistence.py\", line 1409, in <listcomp>\n",
      "    eval_condition(obj)]\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\evaluator.py\", line 121, in evaluate\n",
      "    left_val = eval_left(obj)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\evaluator.py\", line 78, in <lambda>\n",
      "    return lambda obj: get_corresponding_attr(obj)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\attributes.py\", line 242, in __get__\n",
      "    return self.impl.get(instance_state(instance), dict_)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\attributes.py\", line 594, in get\n",
      "    value = state._load_expired(state, passive)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\state.py\", line 608, in _load_expired\n",
      "    self.manager.deferred_scalar_loader(self, toload)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\loading.py\", line 876, in load_scalar_attributes\n",
      "    only_load_props=attribute_names)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\loading.py\", line 188, in load_on_ident\n",
      "    identity_token=identity_token\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\loading.py\", line 250, in load_on_pk_identity\n",
      "    return q.one()\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\query.py\", line 2947, in one\n",
      "    ret = self.one_or_none()\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\query.py\", line 2917, in one_or_none\n",
      "    ret = list(self)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\loading.py\", line 75, in instances\n",
      "    fetch = cursor.fetchall()\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\engine\\result.py\", line 1137, in fetchall\n",
      "    self.cursor, self.context)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 1416, in _handle_dbapi_exception\n",
      "    util.reraise(*exc_info)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\util\\compat.py\", line 249, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\engine\\result.py\", line 1131, in fetchall\n",
      "    l = self.process_rows(self._fetchall_impl())\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\engine\\result.py\", line 1082, in _fetchall_impl\n",
      "    return self.cursor.fetchall()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-40-745653938250>\", line 1, in <module>\n",
      "    update_status = update_db_tweets_table(tweet_list)\n",
      "  File \"<ipython-input-20-2b2c25f4e25b>\", line 70, in update_db_tweets_table\n",
      "    db.session.rollback()\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\scoping.py\", line 153, in do\n",
      "    return getattr(self.registry(), name)(*args, **kwargs)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\session.py\", line 907, in rollback\n",
      "    self.transaction.rollback()\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\session.py\", line 507, in rollback\n",
      "    dirty_only=transaction.nested)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\session.py\", line 364, in _restore_snapshot\n",
      "    s._expire(s.dict, self.session.identity_map._modified)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2978, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1866, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1373, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1281, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1144, in structured_traceback\n",
      "    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\n",
      "TypeError: must be str, not list\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\Jeff\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be str, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-2b2c25f4e25b>\u001b[0m in \u001b[0;36mupdate_db_tweets_table\u001b[1;34m(a_tweet_list)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m                 \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mTweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtweet_id_str\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet_id_str'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m                 \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\query.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, values, synchronize_session, update_args)\u001b[0m\n\u001b[0;32m   3478\u001b[0m             self, synchronize_session, values, update_args)\n\u001b[1;32m-> 3479\u001b[1;33m         \u001b[0mupdate_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexec_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3480\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdate_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrowcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\persistence.py\u001b[0m in \u001b[0;36mexec_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_pre\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1325\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_pre_synchronize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1326\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_exec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\persistence.py\u001b[0m in \u001b[0;36m_do_pre_synchronize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1406\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midentity_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m             \u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1408\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_cls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\persistence.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1408\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_cls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m             eval_condition(obj)]\n\u001b[0m\u001b[0;32m   1410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\evaluator.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m                 \u001b[0mleft_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_left\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m                 \u001b[0mright_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_right\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\evaluator.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mget_corresponding_attr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mget_corresponding_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\attributes.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\attributes.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, state, dict_, passive)\u001b[0m\n\u001b[0;32m    593\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpired_attributes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_expired\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\state.py\u001b[0m in \u001b[0;36m_load_expired\u001b[1;34m(self, state, passive)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeferred_scalar_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\loading.py\u001b[0m in \u001b[0;36mload_scalar_attributes\u001b[1;34m(mapper, state, attribute_names)\u001b[0m\n\u001b[0;32m    875\u001b[0m             \u001b[0mrefresh_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m             only_load_props=attribute_names)\n\u001b[0m\u001b[0;32m    877\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\loading.py\u001b[0m in \u001b[0;36mload_on_ident\u001b[1;34m(query, key, refresh_state, with_for_update, only_load_props)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0monly_load_props\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0monly_load_props\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0midentity_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midentity_token\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m     )\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\loading.py\u001b[0m in \u001b[0;36mload_on_pk_identity\u001b[1;34m(query, primary_key_identity, refresh_state, with_for_update, only_load_props, identity_token)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0morm_exc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoResultFound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\query.py\u001b[0m in \u001b[0;36mone\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2946\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2947\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_or_none\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2948\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0morm_exc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultipleResultsFound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\query.py\u001b[0m in \u001b[0;36mone_or_none\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2916\u001b[0m         \"\"\"\n\u001b[1;32m-> 2917\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\orm\\loading.py\u001b[0m in \u001b[0;36minstances\u001b[1;34m(query, cursor, context)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[0mfetch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\engine\\result.py\u001b[0m in \u001b[0;36mfetchall\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1136\u001b[0m                 \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1137\u001b[1;33m                 self.cursor, self.context)\n\u001b[0m\u001b[0;32m   1138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[0;32m   1415\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1416\u001b[1;33m                 \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\util\\compat.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb, cause)\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\engine\\result.py\u001b[0m in \u001b[0;36mfetchall\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1130\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m             \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_rows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetchall_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_soft_close\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\sqlalchemy\\engine\\result.py\u001b[0m in \u001b[0;36m_fetchall_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1081\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   1862\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1863\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1864\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result)\u001b[0m\n\u001b[0;32m   2976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2977\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2978\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunning_compiled_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2979\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2980\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   1864\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1865\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 1866\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   1867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1868\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1371\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1373\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1279\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1281\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1282\u001b[0m             )\n\u001b[0;32m   1283\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1142\u001b[0m         \u001b[0mexception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parts_of_chained_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1144\u001b[1;33m             \u001b[0mformatted_exceptions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_chained_exception_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__cause__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1145\u001b[0m             \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be str, not list"
     ]
    }
   ],
   "source": [
    "update_status = update_db_tweets_table(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14846\n"
     ]
    }
   ],
   "source": [
    "abc = db.session.query(Tweet).order_by( Tweet.tweet_search_term ).all()\n",
    "print(len(abc))\n",
    "# for r in abc:\n",
    "#     pprint(f\"{r.tweet_search_term}: {r.tweet_id_str} [{r.updated_at}] => {r.tweet_user_screen_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.create_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code From: app.py - Flask app routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************\n",
    "# Default route - display the main page\n",
    "# NOTE: Flask expects rendered templates to be in the ./templates folder\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************\n",
    "# Return information relevant to update\n",
    "# of the 'locations' and 'trends' database tables\n",
    "@app.route(\"/update\")\n",
    "def update_info():\n",
    "    # Obtain remaining number of API calls for trends/place\n",
    "    api_calls_remaining_place = api_calls_remaining( \"place\")\n",
    "\n",
    "    # Obtain time before rate limits are reset for trends/available\n",
    "    api_time_before_reset_place = api_time_before_reset( \"place\")\n",
    "\n",
    "    # Obtain remaining number of API calls for trends/place\n",
    "    api_calls_remaining_available = api_calls_remaining( \"available\")\n",
    "\n",
    "    # Obtain time before rate limits are reset for trends/available\n",
    "    api_time_before_reset_available = api_time_before_reset( \"available\")\n",
    "\n",
    "    # Count the number of locations in the 'locations' table\n",
    "    n_locations = db.session.query(Location).count()\n",
    "\n",
    "    # Count the number of total trends in the 'trends' table\n",
    "    n_trends = db.session.query(Trend).count()\n",
    "\n",
    "    # Provide the average number of Twitter Trends provided per location\n",
    "    # Use try/except to catch divide by zero\n",
    "    try:\n",
    "        n_trends_per_location_avg = n_trends / n_locations\n",
    "    except ZeroDivisionError:\n",
    "        n_trends_per_location_avg = None\n",
    "\n",
    "    api_info = {\n",
    "        'api_calls_remaining_place': api_calls_remaining_place,\n",
    "        'api_time_before_reset_place': api_time_before_reset_place,\n",
    "        'api_calls_remaining_available': api_calls_remaining_available,\n",
    "        'api_time_before_reset_available': api_time_before_reset_available,\n",
    "        'n_locations': n_locations,\n",
    "        'n_trends': n_trends,\n",
    "        'n_trends_per_location_avg' : n_trends_per_location_avg\n",
    "    }\n",
    "\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:    return jsonify(api_info)\n",
    "# CODE TO RUN IN JUPYTER NOTEBOOK\n",
    "    return (api_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************\n",
    "# Return information relevant to update\n",
    "# of the 'locations' and 'trends' database tables\n",
    "@app.route(\"/update/other\")\n",
    "def update_info_other():\n",
    "    # Obtain the full set rate limits info\n",
    "    api_info = api_rate_limits()\n",
    "\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:    return jsonify(api_info)\n",
    "# CODE TO RUN IN JUPYTER NOTEBOOK\n",
    "    return (api_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************\n",
    "# Update the 'locations' table via API calls\n",
    "# Note: Typically requires less than 1 minute\n",
    "@app.route(\"/update/locations\")\n",
    "def update_locations_table():\n",
    "    # Update the locations table through API calls\n",
    "    n_locations = update_db_locations_table()\n",
    "\n",
    "    api_info = {\n",
    "        'n_locations': n_locations\n",
    "    }\n",
    "\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:    return jsonify(api_info)\n",
    "# CODE TO RUN IN JUPYTER NOTEBOOK\n",
    "    return (api_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************\n",
    "# Update the 'trends' table via API calls\n",
    "# Note: Typically requires less than 1 minute if no rate limits\n",
    "#       But require up to 15 minutes if rate limits are in effect\n",
    "@app.route(\"/update/trends\")\n",
    "def update_trends_table():\n",
    "    # Update the trends table through API calls\n",
    "    n_location_trends = update_db_trends_table()\n",
    "\n",
    "    api_info = {\n",
    "        'n_location_trends': n_location_trends\n",
    "    }\n",
    "\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:    return jsonify(api_info)\n",
    "# CODE TO RUN IN JUPYTER NOTEBOOK\n",
    "    return (api_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************\n",
    "# Return a list of all locations with Twitter Top Trend info\n",
    "@app.route(\"/locations\")\n",
    "def get_all_locations():\n",
    "    # Query to obtain all locations in the 'locations' table\n",
    "    # REVISED FOR GeoTweet+: Needs to account for retention of locations over time\n",
    "    # results = db.session.query(Location).all()\n",
    "        \n",
    "    # Create a subquery to find the most recent \"updated_at\" record per woeid\n",
    "    loc_subq = db.session.query(Location.woeid, func.max(Location.updated_at).label(\"max_updated_at\")) \\\n",
    "                            .group_by(Location.woeid).subquery()\n",
    "\n",
    "    results = db.session.query(Location) \\\n",
    "                            .filter( and_( \\\n",
    "                                Location.woeid == loc_subq.c.woeid, \\\n",
    "                                Location.updated_at == loc_subq.c.max_updated_at \\\n",
    "                            )).order_by(Location.woeid).all()\n",
    "\n",
    "    loc_list = []\n",
    "    for r in results:\n",
    "        loc_info = {\n",
    "            'updated_at': r.updated_at,\n",
    "            'woeid': r.woeid,\n",
    "            'latitude': r.latitude,\n",
    "            'longitude': r.longitude,\n",
    "            'name_full': r.name_full,\n",
    "            'name_only': r.name_only,\n",
    "            'name_woe': r.name_woe,\n",
    "            'county_name': r.county_name,\n",
    "            'county_name_only': r.county_name_only,\n",
    "            'county_woeid': r.county_woeid,\n",
    "            'state_name': r.state_name,\n",
    "            'state_name_only': r.state_name_only,\n",
    "            'state_woeid': r.state_woeid,\n",
    "            'country_name': r.country_name,\n",
    "            'country_name_only': r.country_name_only,\n",
    "            'country_woeid': r.country_woeid,\n",
    "            'place_type': r.place_type,\n",
    "            'timezone': r.timezone,\n",
    "            'twitter_type': r.twitter_type,\n",
    "            'twitter_country': r.twitter_country,\n",
    "            'tritter_country_code': r.tritter_country_code,\n",
    "            'twitter_name': r.twitter_name,\n",
    "            'twitter_parentid': r.twitter_parentid\n",
    "        }\n",
    "\n",
    "        # loc_info = {\n",
    "        #     'woeid': r.Location.woeid,\n",
    "        #     'latitude': r.Location.latitude,\n",
    "        #     'longitude': r.Location.longitude,\n",
    "        #     'name_full': r.Location.name_full,\n",
    "        #     'name_only': r.Location.name_only,\n",
    "        #     'name_woe': r.Location.name_woe,\n",
    "        #     'county_name': r.Location.county_name,\n",
    "        #     'county_name_only': r.Location.county_name_only,\n",
    "        #     'county_woeid': r.Location.county_woeid,\n",
    "        #     'state_name': r.Location.state_name,\n",
    "        #     'state_name_only': r.Location.state_name_only,\n",
    "        #     'state_woeid': r.Location.state_woeid,\n",
    "        #     'country_name': r.Location.country_name,\n",
    "        #     'country_name_only': r.Location.country_name_only,\n",
    "        #     'country_woeid': r.Location.country_woeid,\n",
    "        #     'place_type': r.Location.place_type,\n",
    "        #     'timezone': r.Location.timezone,\n",
    "        #     'twitter_type': r.Location.twitter_type,\n",
    "        #     'twitter_country': r.Location.twitter_country,\n",
    "        #     'tritter_country_code': r.Location.tritter_country_code,\n",
    "        #     'twitter_parentid': r.Location.twitter_parentid,\n",
    "\n",
    "        #     'twitter_as_of': r.Trend.twitter_as_of,\n",
    "        #     'twitter_created_at': r.Trend.twitter_created_at,\n",
    "        #     'twitter_name': r.Trend.twitter_name,\n",
    "        #     'twitter_tweet_name': r.Trend.twitter_tweet_name,\n",
    "        #     'twitter_tweet_promoted_content': r.Trend.twitter_tweet_promoted_content,\n",
    "        #     'twitter_tweet_query': r.Trend.twitter_tweet_query,\n",
    "        #     'twitter_tweet_url': r.Trend.twitter_tweet_url,\n",
    "        #     'twitter_tweet_volume': r.Trend.twitter_tweet_volume\n",
    "        # }\n",
    "\n",
    "        loc_list.append(loc_info)\n",
    "\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:    return jsonify(loc_list)\n",
    "# CODE TO RUN IN JUPYTER NOTEBOOK\n",
    "    return (loc_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************\n",
    "# Return a list of all locations with Twitter Top Trend info\n",
    "@app.route(\"/locations/interval/<a_date_range>\")\n",
    "def get_interval_all_locations(a_date_range):\n",
    "    # Query to obtain all locations in the 'locations' table\n",
    "    # for which 'updated_at' is within the specified date range\n",
    "    #     a_date_range = \"2019-03-01\"             ->   \">= 3/1/19\"\n",
    "    #     a_date_range = \":2019-06-01\"            ->   \"<= 6/30/19\"\n",
    "    #     a_date_range = \"2019-03-01:2019-06-30\"  ->   \">= 3/1/19 and  <= 6/30/19\"\n",
    "    #     a_date_range = \"all\"                    ->    all dates\n",
    "    #     a_date_range = \":\"                      ->    same as \"all\"\n",
    "    #     a_date_range = \"\"                       ->    same as \"all\"\n",
    "\n",
    "    \n",
    "    # Parse the date range\n",
    "    q_start_date, q_end_date = parse_date_range(a_date_range)\n",
    "    \n",
    "    # Return with an error if there was a problem parsing the date range\n",
    "    if q_start_date == \"ERROR\" or q_end_date == \"ERROR\":\n",
    "        loc_list = [{'ERROR': 'ERROR'}]\n",
    "        return jsonify(loc_list)\n",
    "    \n",
    "    results = db.session.query(Location) \\\n",
    "                            .filter( and_( \\\n",
    "                                func.date(Location.updated_at) >= q_start_date, \\\n",
    "                                func.date(Location.updated_at) <= q_end_date \\\n",
    "                            )).order_by(Location.woeid).all()\n",
    "\n",
    "    loc_list = []\n",
    "    for r in results:\n",
    "        loc_info = {\n",
    "            'updated_at': r.updated_at,\n",
    "            'woeid': r.woeid,\n",
    "            'latitude': r.latitude,\n",
    "            'longitude': r.longitude,\n",
    "            'name_full': r.name_full,\n",
    "            'name_only': r.name_only,\n",
    "            'name_woe': r.name_woe,\n",
    "            'county_name': r.county_name,\n",
    "            'county_name_only': r.county_name_only,\n",
    "            'county_woeid': r.county_woeid,\n",
    "            'state_name': r.state_name,\n",
    "            'state_name_only': r.state_name_only,\n",
    "            'state_woeid': r.state_woeid,\n",
    "            'country_name': r.country_name,\n",
    "            'country_name_only': r.country_name_only,\n",
    "            'country_woeid': r.country_woeid,\n",
    "            'place_type': r.place_type,\n",
    "            'timezone': r.timezone,\n",
    "            'twitter_type': r.twitter_type,\n",
    "            'twitter_country': r.twitter_country,\n",
    "            'tritter_country_code': r.tritter_country_code,\n",
    "            'twitter_name': r.twitter_name,\n",
    "            'twitter_parentid': r.twitter_parentid\n",
    "        }\n",
    "\n",
    "        loc_list.append(loc_info)\n",
    "\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:    return jsonify(loc_list)\n",
    "# CODE TO RUN IN JUPYTER NOTEBOOK\n",
    "    return (loc_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************\n",
    "# Return a list of one location  with Twitter Top Trend info with teh specified WOEID\n",
    "@app.route(\"/locations/<a_woeid>\")\n",
    "def get_info_for_location(a_woeid):\n",
    "    # Query to obtain all locations in the 'locations' table\n",
    "    # REVISED FOR GeoTweet+: Needs to account for retention of locations over time\n",
    "    # results = db.session.query(Location) \\\n",
    "    #                     .filter(Location.woeid == a_woeid) \\\n",
    "    #                     .all()\n",
    "        \n",
    "    # Create a subquery to find the most recent \"updated_at\" record per woeid\n",
    "    loc_subq = db.session.query(Location.woeid, func.max(Location.updated_at).label(\"max_updated_at\")) \\\n",
    "                            .group_by(Location.woeid).subquery()\n",
    "\n",
    "    results = db.session.query(Location) \\\n",
    "                            .filter( and_( \\\n",
    "                                Location.woeid == a_woeid, \\\n",
    "                                Location.woeid == loc_subq.c.woeid, \\\n",
    "                                Location.updated_at == loc_subq.c.max_updated_at \\\n",
    "                            )).order_by(Location.woeid).all()\n",
    "    \n",
    "    loc_list = []\n",
    "    for r in results:\n",
    "        loc_info = {\n",
    "            'updated_at': r.updated_at,\n",
    "            'woeid': r.woeid,\n",
    "            'latitude': r.latitude,\n",
    "            'longitude': r.longitude,\n",
    "            'name_full': r.name_full,\n",
    "            'name_only': r.name_only,\n",
    "            'name_woe': r.name_woe,\n",
    "            'county_name': r.county_name,\n",
    "            'county_name_only': r.county_name_only,\n",
    "            'county_woeid': r.county_woeid,\n",
    "            'state_name': r.state_name,\n",
    "            'state_name_only': r.state_name_only,\n",
    "            'state_woeid': r.state_woeid,\n",
    "            'country_name': r.country_name,\n",
    "            'country_name_only': r.country_name_only,\n",
    "            'country_woeid': r.country_woeid,\n",
    "            'place_type': r.place_type,\n",
    "            'timezone': r.timezone,\n",
    "            'twitter_type': r.twitter_type,\n",
    "            'twitter_country': r.twitter_country,\n",
    "            'tritter_country_code': r.tritter_country_code,\n",
    "            'twitter_name': r.twitter_name,\n",
    "            'twitter_parentid': r.twitter_parentid\n",
    "        }\n",
    "\n",
    "        loc_list.append(loc_info)\n",
    "\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:    return jsonify(loc_list)\n",
    "# CODE TO RUN IN JUPYTER NOTEBOOK\n",
    "    return (loc_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************\n",
    "# Return a list of all locations with Twitter Top Trend info\n",
    "@app.route(\"/locations/interval/<a_date_range>/<a_woeid>\")\n",
    "def get_interval_info_for_location(a_date_range, a_woeid):\n",
    "    # Query to obtain all locations in the 'locations' table\n",
    "    # for which 'updated_at' is within the specified date range\n",
    "    #     a_date_range = \"2019-03-01\"             ->   \">= 3/1/19\"\n",
    "    #     a_date_range = \":2019-06-01\"            ->   \"<= 6/30/19\"\n",
    "    #     a_date_range = \"2019-03-01:2019-06-30\"  ->   \">= 3/1/19 and  <= 6/30/19\"\n",
    "    #     a_date_range = \"all\"                    ->    all dates\n",
    "    #     a_date_range = \":\"                      ->    same as \"all\"\n",
    "    #     a_date_range = \"\"                       ->    same as \"all\"\n",
    "\n",
    "    \n",
    "    # Parse the date range\n",
    "    q_start_date, q_end_date = parse_date_range(a_date_range)\n",
    "    \n",
    "    # Return with an error if there was a problem parsing the date range\n",
    "    if q_start_date == \"ERROR\" or q_end_date == \"ERROR\":\n",
    "        loc_list = [{'ERROR': 'ERROR'}]\n",
    "        return jsonify(loc_list)\n",
    "    \n",
    "    results = db.session.query(Location) \\\n",
    "                            .filter( and_( \\\n",
    "                                Location.woeid == a_woeid, \\\n",
    "                                func.date(Location.updated_at) >= q_start_date, \\\n",
    "                                func.date(Location.updated_at) <= q_end_date \\\n",
    "                            )).order_by(Location.woeid).all()\n",
    "\n",
    "    loc_list = []\n",
    "    for r in results:\n",
    "        loc_info = {\n",
    "            'updated_at': r.updated_at,\n",
    "            'woeid': r.woeid,\n",
    "            'latitude': r.latitude,\n",
    "            'longitude': r.longitude,\n",
    "            'name_full': r.name_full,\n",
    "            'name_only': r.name_only,\n",
    "            'name_woe': r.name_woe,\n",
    "            'county_name': r.county_name,\n",
    "            'county_name_only': r.county_name_only,\n",
    "            'county_woeid': r.county_woeid,\n",
    "            'state_name': r.state_name,\n",
    "            'state_name_only': r.state_name_only,\n",
    "            'state_woeid': r.state_woeid,\n",
    "            'country_name': r.country_name,\n",
    "            'country_name_only': r.country_name_only,\n",
    "            'country_woeid': r.country_woeid,\n",
    "            'place_type': r.place_type,\n",
    "            'timezone': r.timezone,\n",
    "            'twitter_type': r.twitter_type,\n",
    "            'twitter_country': r.twitter_country,\n",
    "            'tritter_country_code': r.tritter_country_code,\n",
    "            'twitter_name': r.twitter_name,\n",
    "            'twitter_parentid': r.twitter_parentid\n",
    "        }\n",
    "\n",
    "        loc_list.append(loc_info)\n",
    "\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:    return jsonify(loc_list)\n",
    "# CODE TO RUN IN JUPYTER NOTEBOOK\n",
    "    return (loc_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************\n",
    "# Return a list of all locations that have the specified tweet in its top trends\n",
    "# and then sort the results by tweet volume in descending order (with NULLs last)\n",
    "@app.route(\"/locations/tweet/<a_tweet>\")\n",
    "def get_locations_with_tweet(a_tweet):\n",
    "    # Query to obtain all locations in the 'locations' table\n",
    "    # REVISED FOR GeoTweet+: Needs to account for retention of locations over time\n",
    "\n",
    "    # Create a subquery to find the most recent locations table \"updated_at\" record per woeid\n",
    "    loc_subq = db.session.query(Location.woeid, func.max(Location.updated_at).label(\"max_loc_updated_at\")) \\\n",
    "                            .group_by(Location.woeid).subquery()\n",
    "    \n",
    "    # Create a subquery to find the most recent trends table \"updated_at\" record per woeid\n",
    "    trend_subq = db.session.query(Trend.woeid, func.max(Trend.updated_at).label(\"max_trend_updated_at\")) \\\n",
    "                            .group_by(Trend.woeid).subquery() \n",
    "    \n",
    "    results = db.session.query(Trend, Location).join(Location) \\\n",
    "                            .filter( and_( \\\n",
    "                                Trend.twitter_tweet_name == a_tweet, \\\n",
    "                                Trend.woeid == trend_subq.c.woeid, \\\n",
    "                                Trend.updated_at == trend_subq.c.max_trend_updated_at, \\\n",
    "                                Location.woeid == loc_subq.c.woeid, \\\n",
    "                                Location.updated_at == loc_subq.c.max_loc_updated_at \\\n",
    "                                )).order_by( coalesce(Trend.twitter_tweet_volume, -9999).desc() ).all()\n",
    "\n",
    "    loc_list = []\n",
    "    for r in results:\n",
    "        #print(f\"Trend Information for {r.Trend.woeid} {r.Location.name_full}: {r.Trend.twitter_tweet_name} {r.Trend.twitter_tweet_volume}\")\n",
    "        loc_info = {\n",
    "            'loc_updated_at': r.Location.updated_at,\n",
    "            'woeid': r.Location.woeid,\n",
    "            'latitude': r.Location.latitude,\n",
    "            'longitude': r.Location.longitude,\n",
    "            'name_full': r.Location.name_full,\n",
    "            'name_only': r.Location.name_only,\n",
    "            'name_woe': r.Location.name_woe,\n",
    "            'county_name': r.Location.county_name,\n",
    "            'county_name_only': r.Location.county_name_only,\n",
    "            'county_woeid': r.Location.county_woeid,\n",
    "            'state_name': r.Location.state_name,\n",
    "            'state_name_only': r.Location.state_name_only,\n",
    "            'state_woeid': r.Location.state_woeid,\n",
    "            'country_name': r.Location.country_name,\n",
    "            'country_name_only': r.Location.country_name_only,\n",
    "            'country_woeid': r.Location.country_woeid,\n",
    "            'place_type': r.Location.place_type,\n",
    "            'timezone': r.Location.timezone,\n",
    "            'twitter_type': r.Location.twitter_type,\n",
    "            'twitter_country': r.Location.twitter_country,\n",
    "            'tritter_country_code': r.Location.tritter_country_code,\n",
    "            'twitter_parentid': r.Location.twitter_parentid,\n",
    "\n",
    "            'trend_updated_at': r.Trend.updated_at,\n",
    "            'twitter_as_of': r.Trend.twitter_as_of,\n",
    "            'twitter_created_at': r.Trend.twitter_created_at,\n",
    "            'twitter_name': r.Trend.twitter_name,\n",
    "            'twitter_tweet_name': r.Trend.twitter_tweet_name,\n",
    "            'twitter_tweet_promoted_content': r.Trend.twitter_tweet_promoted_content,\n",
    "            'twitter_tweet_query': r.Trend.twitter_tweet_query,\n",
    "            'twitter_tweet_url': r.Trend.twitter_tweet_url,\n",
    "            'twitter_tweet_volume': r.Trend.twitter_tweet_volume\n",
    "        }\n",
    "\n",
    "        loc_list.append(loc_info)\n",
    "\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:    return jsonify(loc_list)\n",
    "# CODE TO RUN IN JUPYTER NOTEBOOK\n",
    "    return (loc_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************\n",
    "# Return a list of all locations that have the specified tweet in its top trends\n",
    "# and then sort the results by tweet volume in descending order (with NULLs last)\n",
    "@app.route(\"/locations/interval/<a_date_range>/tweet/<a_tweet>\")\n",
    "def get_interval_locations_with_tweet(a_date_range, a_tweet):\n",
    "    # Query to obtain all locations in the 'locations' table\n",
    "    # REVISED FOR GeoTweet+: Needs to account for retention of locations over time\n",
    "    #     a_date_range = \"2019-03-01\"             ->   \">= 3/1/19\"\n",
    "    #     a_date_range = \":2019-06-01\"            ->   \"<= 6/30/19\"\n",
    "    #     a_date_range = \"2019-03-01:2019-06-30\"  ->   \">= 3/1/19 and  <= 6/30/19\"\n",
    "    #     a_date_range = \"all\"                    ->    all dates\n",
    "    #     a_date_range = \":\"                      ->    same as \"all\"\n",
    "    #     a_date_range = \"\"                       ->    same as \"all\"\n",
    "\n",
    "    \n",
    "    # Parse the date range\n",
    "    q_start_date, q_end_date = parse_date_range(a_date_range)\n",
    "    \n",
    "    # Return with an error if there was a problem parsing the date range\n",
    "    if q_start_date == \"ERROR\" or q_end_date == \"ERROR\":\n",
    "        trend_list = [{'ERROR': 'ERROR'}]\n",
    "        return jsonify(trend_list)\n",
    "    \n",
    "    # Query to pull all of the most recent Trends (50 per entry in 'locations' table)\n",
    "    # In the order_by clause, use the coalesce() function to replace all NULL values\n",
    "    # in the twitter_tweet_volume field with -9999 for the purpose of the sort in descending order\n",
    "    results = db.session.query(Trend, Location).join(Location) \\\n",
    "                            .filter( and_( \\\n",
    "                                Trend.twitter_tweet_name == a_tweet, \\\n",
    "                                func.date(Trend.updated_at) >= q_start_date, \\\n",
    "                                func.date(Trend.updated_at) <= q_end_date \\\n",
    "                            )).order_by( coalesce(Trend.twitter_tweet_volume, -9999).desc() ).all()\n",
    "\n",
    "    loc_list = []\n",
    "    for r in results:\n",
    "        #print(f\"Trend Information for {r.Trend.woeid} {r.Location.name_full}: {r.Trend.twitter_tweet_name} {r.Trend.twitter_tweet_volume}\")\n",
    "        loc_info = {\n",
    "            'loc_updated_at': r.Location.updated_at,\n",
    "            'woeid': r.Location.woeid,\n",
    "            'latitude': r.Location.latitude,\n",
    "            'longitude': r.Location.longitude,\n",
    "            'name_full': r.Location.name_full,\n",
    "            'name_only': r.Location.name_only,\n",
    "            'name_woe': r.Location.name_woe,\n",
    "            'county_name': r.Location.county_name,\n",
    "            'county_name_only': r.Location.county_name_only,\n",
    "            'county_woeid': r.Location.county_woeid,\n",
    "            'state_name': r.Location.state_name,\n",
    "            'state_name_only': r.Location.state_name_only,\n",
    "            'state_woeid': r.Location.state_woeid,\n",
    "            'country_name': r.Location.country_name,\n",
    "            'country_name_only': r.Location.country_name_only,\n",
    "            'country_woeid': r.Location.country_woeid,\n",
    "            'place_type': r.Location.place_type,\n",
    "            'timezone': r.Location.timezone,\n",
    "            'twitter_type': r.Location.twitter_type,\n",
    "            'twitter_country': r.Location.twitter_country,\n",
    "            'tritter_country_code': r.Location.tritter_country_code,\n",
    "            'twitter_parentid': r.Location.twitter_parentid,\n",
    "\n",
    "            'trend_updated_at': r.Trend.updated_at,\n",
    "            'twitter_as_of': r.Trend.twitter_as_of,\n",
    "            'twitter_created_at': r.Trend.twitter_created_at,\n",
    "            'twitter_name': r.Trend.twitter_name,\n",
    "            'twitter_tweet_name': r.Trend.twitter_tweet_name,\n",
    "            'twitter_tweet_promoted_content': r.Trend.twitter_tweet_promoted_content,\n",
    "            'twitter_tweet_query': r.Trend.twitter_tweet_query,\n",
    "            'twitter_tweet_url': r.Trend.twitter_tweet_url,\n",
    "            'twitter_tweet_volume': r.Trend.twitter_tweet_volume\n",
    "        }\n",
    "\n",
    "        loc_list.append(loc_info)\n",
    "\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:    return jsonify(loc_list)\n",
    "# CODE TO RUN IN JUPYTER NOTEBOOK\n",
    "    return (loc_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************\n",
    "# Return the full list of all trends with Twitter Top Trend info\n",
    "@app.route(\"/trends\")\n",
    "def get_all_trends():\n",
    "    # Query to obtain all trends in the 'trends' table\n",
    "    # REVISED FOR GeoTweet+: Needs to account for retention of trends over time\n",
    "    # results = db.session.query(Trend).all()\n",
    "\n",
    "    # Create a subquery to find the most recent \"updated_at\" record per woeid\n",
    "    trend_subq = db.session.query(Trend.woeid, func.max(Trend.updated_at).label(\"max_updated_at\")) \\\n",
    "                                .group_by(Trend.woeid).subquery()\n",
    "\n",
    "    # Query to pull all of the most recent Trends (50 per entry in 'locations' table)\n",
    "    results = db.session.query(Trend) \\\n",
    "                            .filter( and_(\n",
    "                                    Trend.woeid == trend_subq.c.woeid, \\\n",
    "                                    Trend.updated_at == trend_subq.c.max_updated_at \\\n",
    "                            )).order_by( coalesce(Trend.twitter_tweet_volume, -9999).desc() ).all()\n",
    "\n",
    "    \n",
    "    trend_list = []\n",
    "    for r in results:\n",
    "        trend_info = {\n",
    "            'updated_at': r.updated_at,\n",
    "            'woeid': r.woeid,\n",
    "            'twitter_as_of': r.twitter_as_of,\n",
    "            'twitter_created_at': r.twitter_created_at,\n",
    "            'twitter_name': r.twitter_name,\n",
    "            'twitter_tweet_name': r.twitter_tweet_name,\n",
    "            'twitter_tweet_promoted_content': r.twitter_tweet_promoted_content,\n",
    "            'twitter_tweet_query': r.twitter_tweet_query,\n",
    "            'twitter_tweet_url': r.twitter_tweet_url,\n",
    "            'twitter_tweet_volume': r.twitter_tweet_volume\n",
    "        }\n",
    "\n",
    "        trend_list.append(trend_info)\n",
    "\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:    return jsonify(trend_list)\n",
    "# CODE TO RUN IN JUPYTER NOTEBOOK\n",
    "    return (trend_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************\n",
    "# Return the full list of all trends with Twitter Top Trend info\n",
    "@app.route(\"/trends/interval/<a_date_range>\")\n",
    "def get_interval_all_trends(a_date_range):\n",
    "    # Query to obtain all trends in the 'trends' table\n",
    "    # for which 'updated_at' is within the specified date range\n",
    "    #     a_date_range = \"2019-03-01\"             ->   \">= 3/1/19\"\n",
    "    #     a_date_range = \":2019-06-01\"            ->   \"<= 6/30/19\"\n",
    "    #     a_date_range = \"2019-03-01:2019-06-30\"  ->   \">= 3/1/19 and  <= 6/30/19\"\n",
    "    #     a_date_range = \"all\"                    ->    all dates\n",
    "    #     a_date_range = \":\"                      ->    same as \"all\"\n",
    "    #     a_date_range = \"\"                       ->    same as \"all\"\n",
    "\n",
    "    \n",
    "    # Parse the date range\n",
    "    q_start_date, q_end_date = parse_date_range(a_date_range)\n",
    "    \n",
    "    # Return with an error if there was a problem parsing the date range\n",
    "    if q_start_date == \"ERROR\" or q_end_date == \"ERROR\":\n",
    "        trend_list = [{'ERROR': 'ERROR'}]\n",
    "        return jsonify(trend_list)\n",
    "    \n",
    "    # Query to pull all of the most recent Trends (50 per entry in 'locations' table)\n",
    "    results = db.session.query(Trend) \\\n",
    "                            .filter( and_( \\\n",
    "                                func.date(Trend.updated_at) >= q_start_date, \\\n",
    "                                func.date(Trend.updated_at) <= q_end_date \\\n",
    "                            )).order_by( coalesce(Trend.twitter_tweet_volume, -9999).desc() ).all()\n",
    "    \n",
    "    trend_list = []\n",
    "    for r in results:\n",
    "        trend_info = {\n",
    "            'updated_at': r.updated_at,\n",
    "            'woeid': r.woeid,\n",
    "            'twitter_as_of': r.twitter_as_of,\n",
    "            'twitter_created_at': r.twitter_created_at,\n",
    "            'twitter_name': r.twitter_name,\n",
    "            'twitter_tweet_name': r.twitter_tweet_name,\n",
    "            'twitter_tweet_promoted_content': r.twitter_tweet_promoted_content,\n",
    "            'twitter_tweet_query': r.twitter_tweet_query,\n",
    "            'twitter_tweet_url': r.twitter_tweet_url,\n",
    "            'twitter_tweet_volume': r.twitter_tweet_volume\n",
    "        }\n",
    "\n",
    "        trend_list.append(trend_info)\n",
    "\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:    return jsonify(trend_list)\n",
    "# CODE TO RUN IN JUPYTER NOTEBOOK\n",
    "    return (trend_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************\n",
    "# Return the full list of Twitter Top Trends for a specific location\n",
    "# and then sort the results by tweet volume in descending order (with NULLs last)\n",
    "@app.route(\"/trends/<a_woeid>\")\n",
    "def get_trends_for_location(a_woeid):\n",
    "    # Query to obtain all trends in the 'trends' table\n",
    "    # REVISED FOR GeoTweet+: Needs to account for retention of trends over time\n",
    "    # results = db.session.query(Trend).filter(Trend.woeid == a_woeid) \\\n",
    "    #                    .order_by(Trend.twitter_tweet_volume.desc().nullslast() ) \\\n",
    "    #                    .all()\n",
    "\n",
    "    # Create a subquery to find the most recent \"updated_at\" record per woeid\n",
    "    trend_subq = db.session.query(Trend.woeid, func.max(Trend.updated_at).label(\"max_updated_at\")) \\\n",
    "                                .group_by(Trend.woeid).subquery()\n",
    "\n",
    "    # Query to pull all of the most recent Trends (50 per entry in 'locations' table)\n",
    "    results = db.session.query(Trend) \\\n",
    "                            .filter( and_( \\\n",
    "                                Trend.woeid == a_woeid, \\\n",
    "                                Trend.woeid == trend_subq.c.woeid, \\\n",
    "                                Trend.updated_at == trend_subq.c.max_updated_at \\\n",
    "                            )).order_by( coalesce(Trend.twitter_tweet_volume, -9999).desc() ).all()\n",
    "    \n",
    "    trend_list = []\n",
    "    for r in results:\n",
    "        trend_info = {\n",
    "            'updated_at': r.updated_at,\n",
    "            'woeid': r.woeid,\n",
    "            'twitter_as_of': r.twitter_as_of,\n",
    "            'twitter_created_at': r.twitter_created_at,\n",
    "            'twitter_name': r.twitter_name,\n",
    "            'twitter_tweet_name': r.twitter_tweet_name,\n",
    "            'twitter_tweet_promoted_content': r.twitter_tweet_promoted_content,\n",
    "            'twitter_tweet_query': r.twitter_tweet_query,\n",
    "            'twitter_tweet_url': r.twitter_tweet_url,\n",
    "            'twitter_tweet_volume': r.twitter_tweet_volume\n",
    "        }\n",
    "\n",
    "        trend_list.append(trend_info)\n",
    "\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:    return jsonify(trend_list)\n",
    "# CODE TO RUN IN JUPYTER NOTEBOOK\n",
    "    return (trend_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************\n",
    "# Return the full list of all trends with Twitter Top Trend info\n",
    "@app.route(\"/trends/interval/<a_date_range>/<a_woeid>\")\n",
    "def get_interval_trends_for_location(a_date_range, a_woeid):\n",
    "    # Query to obtain all trends in the 'trends' table\n",
    "    # for which 'updated_at' is within the specified date range\n",
    "    #     a_date_range = \"2019-03-01\"             ->   \">= 3/1/19\"\n",
    "    #     a_date_range = \":2019-06-01\"            ->   \"<= 6/30/19\"\n",
    "    #     a_date_range = \"2019-03-01:2019-06-30\"  ->   \">= 3/1/19 and  <= 6/30/19\"\n",
    "    #     a_date_range = \"all\"                    ->    all dates\n",
    "    #     a_date_range = \":\"                      ->    same as \"all\"\n",
    "    #     a_date_range = \"\"                       ->    same as \"all\"\n",
    "\n",
    "    \n",
    "    # Parse the date range\n",
    "    q_start_date, q_end_date = parse_date_range(a_date_range)\n",
    "    \n",
    "    # Return with an error if there was a problem parsing the date range\n",
    "    if q_start_date == \"ERROR\" or q_end_date == \"ERROR\":\n",
    "        trend_list = [{'ERROR': 'ERROR'}]\n",
    "        return jsonify(trend_list)\n",
    "    \n",
    "    # Query to pull all of the most recent Trends (50 per entry in 'locations' table)\n",
    "    results = db.session.query(Trend) \\\n",
    "                            .filter( and_( \\\n",
    "                                Trend.woeid == a_woeid, \\\n",
    "                                func.date(Trend.updated_at) >= q_start_date, \\\n",
    "                                func.date(Trend.updated_at) <= q_end_date \\\n",
    "                            )).order_by( coalesce(Trend.twitter_tweet_volume, -9999).desc() ).all()\n",
    "\n",
    "    \n",
    "    trend_list = []\n",
    "    for r in results:\n",
    "        trend_info = {\n",
    "            'updated_at': r.updated_at,\n",
    "            'woeid': r.woeid,\n",
    "            'twitter_as_of': r.twitter_as_of,\n",
    "            'twitter_created_at': r.twitter_created_at,\n",
    "            'twitter_name': r.twitter_name,\n",
    "            'twitter_tweet_name': r.twitter_tweet_name,\n",
    "            'twitter_tweet_promoted_content': r.twitter_tweet_promoted_content,\n",
    "            'twitter_tweet_query': r.twitter_tweet_query,\n",
    "            'twitter_tweet_url': r.twitter_tweet_url,\n",
    "            'twitter_tweet_volume': r.twitter_tweet_volume\n",
    "        }\n",
    "\n",
    "        trend_list.append(trend_info)\n",
    "\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:    return jsonify(trend_list)\n",
    "# CODE TO RUN IN JUPYTER NOTEBOOK\n",
    "    return (trend_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************\n",
    "# Return the top 5 list of Twitter Top Trends for a specific location\n",
    "# and then sort the results by tweet volume in descending order (with NULLs last)\n",
    "@app.route(\"/trends/top/<a_woeid>\")\n",
    "def get_top_trends_for_location(a_woeid):\n",
    "    # Query to obtain all trends in the 'trends' table\n",
    "    # REVISED FOR GeoTweet+: Needs to account for retention of trends over time\n",
    "\n",
    "    # Create a subquery to find the most recent \"updated_at\" record per woeid\n",
    "    trend_subq = db.session.query(Trend.woeid, func.max(Trend.updated_at).label(\"max_updated_at\")) \\\n",
    "                                .group_by(Trend.woeid).subquery()\n",
    "\n",
    "    results = db.session.query(Trend) \\\n",
    "                            .filter( and_( \\\n",
    "                                Trend.woeid == a_woeid, \\\n",
    "                                Trend.woeid == trend_subq.c.woeid, \\\n",
    "                                Trend.updated_at == trend_subq.c.max_updated_at \\\n",
    "                            )).order_by( coalesce(Trend.twitter_tweet_volume, -9999).desc() ).limit(10).all()\n",
    "\n",
    "    trend_list = []\n",
    "    for r in results:\n",
    "        trend_info = {\n",
    "            'updated_at': r.updated_at,\n",
    "            'woeid': r.woeid,\n",
    "            'twitter_as_of': r.twitter_as_of,\n",
    "            'twitter_created_at': r.twitter_created_at,\n",
    "            'twitter_name': r.twitter_name,\n",
    "            'twitter_tweet_name': r.twitter_tweet_name,\n",
    "            'twitter_tweet_promoted_content': r.twitter_tweet_promoted_content,\n",
    "            'twitter_tweet_query': r.twitter_tweet_query,\n",
    "            'twitter_tweet_url': r.twitter_tweet_url,\n",
    "            'twitter_tweet_volume': r.twitter_tweet_volume\n",
    "        }\n",
    "\n",
    "        trend_list.append(trend_info)\n",
    "\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:    return jsonify(trend_list)\n",
    "# CODE TO RUN IN JUPYTER NOTEBOOK\n",
    "    return (trend_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************\n",
    "# Return the full list of all trends with Twitter Top Trend info\n",
    "@app.route(\"/trends/interval/<a_date_range>/top/<a_woeid>\")\n",
    "def get_interval_top_trends_for_location(a_date_range, a_woeid):\n",
    "    # Query to obtain all trends in the 'trends' table\n",
    "    # for which 'updated_at' is within the specified date range\n",
    "    #     a_date_range = \"2019-03-01\"             ->   \">= 3/1/19\"\n",
    "    #     a_date_range = \":2019-06-01\"            ->   \"<= 6/30/19\"\n",
    "    #     a_date_range = \"2019-03-01:2019-06-30\"  ->   \">= 3/1/19 and  <= 6/30/19\"\n",
    "    #     a_date_range = \"all\"                    ->    all dates\n",
    "    #     a_date_range = \":\"                      ->    same as \"all\"\n",
    "    #     a_date_range = \"\"                       ->    same as \"all\"\n",
    "\n",
    "    \n",
    "    # Parse the date range\n",
    "    q_start_date, q_end_date = parse_date_range(a_date_range)\n",
    "    \n",
    "    # Return with an error if there was a problem parsing the date range\n",
    "    if q_start_date == \"ERROR\" or q_end_date == \"ERROR\":\n",
    "        trend_list = [{'ERROR': 'ERROR'}]\n",
    "        return jsonify(trend_list)\n",
    "    \n",
    "    # Query to pull all of the most recent Trends (50 per entry in 'locations' table)\n",
    "    results = db.session.query(Trend) \\\n",
    "                            .filter( and_( \\\n",
    "                                Trend.woeid == a_woeid, \\\n",
    "                                func.date(Trend.updated_at) >= q_start_date, \\\n",
    "                                func.date(Trend.updated_at) <= q_end_date \\\n",
    "                            )).order_by( coalesce(Trend.twitter_tweet_volume, -9999).desc() ).limit(10).all()\n",
    "\n",
    "    trend_list = []\n",
    "    for r in results:\n",
    "        trend_info = {\n",
    "            'updated_at': r.updated_at,\n",
    "            'woeid': r.woeid,\n",
    "            'twitter_as_of': r.twitter_as_of,\n",
    "            'twitter_created_at': r.twitter_created_at,\n",
    "            'twitter_name': r.twitter_name,\n",
    "            'twitter_tweet_name': r.twitter_tweet_name,\n",
    "            'twitter_tweet_promoted_content': r.twitter_tweet_promoted_content,\n",
    "            'twitter_tweet_query': r.twitter_tweet_query,\n",
    "            'twitter_tweet_url': r.twitter_tweet_url,\n",
    "            'twitter_tweet_volume': r.twitter_tweet_volume\n",
    "        }\n",
    "\n",
    "        trend_list.append(trend_info)\n",
    "\n",
    "# COMMENTED OUT TO RUN IN JUPYTER NOTEBOOK:    return jsonify(trend_list)\n",
    "# CODE TO RUN IN JUPYTER NOTEBOOK\n",
    "    return (trend_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Rate Limit Flask Route functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_info_other()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify DB table update functions using Local Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Update locations table\n",
    "# n_locations = update_db_locations_table()\n",
    "# print(n_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Update trends table\n",
    "# n_location_trends = update_db_trends_table()\n",
    "# print(n_location_trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Basic DB functions using Local Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing if support function parse_date_range() is working ok for all input types\n",
    "for a_date_range in [ \"UTC\", None, \"all\", \"\", \":\", \"2019-03-01\", \"2019-03-01:\", \":2019-06-01\", \"2019-03-01:2019-06-30\", \":UTC\"]:\n",
    "    (q_start_date, q_end_date) = parse_date_range(a_date_range)\n",
    "    print(f\"a_date_range: '{a_date_range}' => q_start_date '{q_start_date}', q_end_date '{q_end_date}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Basic DB functions using Local Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all locations\n",
    "retval = get_all_locations()\n",
    "print(len(retval))\n",
    "pprint(retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retval = get_interval_all_locations(\"4/29/19\")\n",
    "print(len(retval))\n",
    "pprint(retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read one location - e.g., 2352824 (Albuquerque)\n",
    "retval = get_info_for_location(2352824)\n",
    "print(len(retval))\n",
    "pprint(retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retval = get_interval_info_for_location(\"4/29/19\",2352824)\n",
    "print(len(retval))\n",
    "pprint(retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read all trends\n",
    "retval = get_all_trends()\n",
    "print(len(retval))\n",
    "pprint(retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all trends\n",
    "retval = get_interval_all_trends(\"4/28/19\")\n",
    "print(len(retval))\n",
    "pprint(retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read trends for one location - e.g., 2352824 (Albuquerque)\n",
    "retval = get_trends_for_location(2352824)\n",
    "print(len(retval))\n",
    "pprint(retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read trends for one location - e.g., 2352824 (Albuquerque)\n",
    "retval = get_interval_trends_for_location(\"4/28/19\", 2352824)\n",
    "print(len(retval))\n",
    "pprint(retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read only the top trends for one location - e.g., 2352824 (Albuquerque)\n",
    "retval = get_top_trends_for_location(2352824)\n",
    "print(len(retval))\n",
    "pprint(retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read only the top trends for one location - e.g., 2352824 (Albuquerque)\n",
    "retval = get_interval_top_trends_for_location(\"4/28/19\", 2352824)\n",
    "print(len(retval))\n",
    "pprint(retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read all locations with specified tweet in its trends list - e.g., \"#SriLanka\"\n",
    "retval = get_locations_with_tweet(\"#AvengersEndgame\")\n",
    "print(len(retval))\n",
    "pprint(retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retval = get_interval_locations_with_tweet(\"\",\"#AvengersEndgame\")\n",
    "print(len(retval))\n",
    "pprint(retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate a new 'tweets' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
